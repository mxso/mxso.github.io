<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.3" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/favicon.ico?v=5.1.3">


  <link rel="icon" type="image/png" sizes="32x32" href="/favicon.ico?v=5.1.3">


  <link rel="icon" type="image/png" sizes="16x16" href="/favicon.ico?v=5.1.3">


  <link rel="mask-icon" href="/favicon.ico?v=5.1.3" color="#222">





  <meta name="keywords" content="Github,DeepLearning,ObjectDetection,Paper," />





  <link rel="alternate" href="/atom.xml" title="Dreamsong's Blog" type="application/atom+xml" />






<meta name="description" content="本文转自handong1587的个人博客，总结的很详细！辛苦了！">
<meta name="keywords" content="Github,DeepLearning,ObjectDetection,Paper">
<meta property="og:type" content="article">
<meta property="og:title" content="Object-Detection">
<meta property="og:url" content="http://songit.cn/Object-Detection/index.html">
<meta property="og:site_name" content="Dreamsong&#39;s Blog">
<meta property="og:description" content="本文转自handong1587的个人博客，总结的很详细！辛苦了！">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="https://camo.githubusercontent.com/e69d4118b20a42de4e23b9549f9a6ec6dbbb0814/687474703a2f2f706a7265646469652e636f6d2f6d656469612f66696c65732f6461726b6e65742d626c61636b2d736d616c6c2e706e67">
<meta property="og:image" content="http://guanghan.info/blog/en/wp-content/uploads/2015/12/images-40.jpg">
<meta property="og:image" content="https://camo.githubusercontent.com/ad9b147ed3a5f48ffb7c3540711c15aa04ce49c6/687474703a2f2f7777772e63732e756e632e6564752f7e776c69752f7061706572732f7373642e706e67">
<meta property="og:image" content="https://user-images.githubusercontent.com/3794909/28934967-718c9302-78b5-11e7-89ee-8b514e53e23c.png">
<meta property="og:image" content="http://cs-people.bu.edu/jmzhang/images/pasted%20image%201465x373.jpg">
<meta property="og:image" content="http://cs-people.bu.edu/jmzhang/images/frontpage.png?crc=123070793">
<meta property="og:image" content="http://personal.ie.cuhk.edu.hk/~ys014/projects/Faceness/support/index.png">
<meta property="og:image" content="https://kpzhang93.github.io/MTCNN_face_detection_alignment/support/index.png">
<meta property="og:image" content="http://songit.cn/assets/object-detection-materials/end_to_end_people_detection_in_crowded_scenes.jpg">
<meta property="og:image" content="https://camo.githubusercontent.com/88a65f132aa4ae4b0477e3ad02c13cdc498377d9/687474703a2f2f37786e37777a2e636f6d312e7a302e676c622e636c6f7564646e2e636f6d2f44656570536b656c65746f6e2e706e673f696d61676556696577322f322f772f353030">
<meta property="og:image" content="https://raw.githubusercontent.com/geometalab/OSMDeepOD/master/imgs/process.png">
<meta property="og:image" content="http://cnnlocalization.csail.mit.edu/framework.jpg">
<meta property="og:image" content="http://www.di.ens.fr/willow/research/contextlocnet/model.png">
<meta property="og:updated_time" content="2018-03-30T09:09:28.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Object-Detection">
<meta name="twitter:description" content="本文转自handong1587的个人博客，总结的很详细！辛苦了！">
<meta name="twitter:image" content="https://camo.githubusercontent.com/e69d4118b20a42de4e23b9549f9a6ec6dbbb0814/687474703a2f2f706a7265646469652e636f6d2f6d656469612f66696c65732f6461726b6e65742d626c61636b2d736d616c6c2e706e67">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.3',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://songit.cn/Object-Detection/"/>





  <title>Object-Detection | Dreamsong's Blog</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Dreamsong's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <h1 class="site-subtitle" itemprop="description">隐居山林，潜心研究。</h1>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-guestbook">
          <a href="/guestbook" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-comment"></i> <br />
            
            留言
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="搜索..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://songit.cn/Object-Detection/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Dreamsong">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Dreamsong's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">Object-Detection</h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-03-03T17:00:58+08:00">
                2018-03-03
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Tech/" itemprop="url" rel="index">
                    <span itemprop="name">Tech</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
              <span class="post-meta-divider">|</span>
              <span class="post-meta-item-icon">
                <i class="fa fa-comment-o"></i>
              </span>
              
                <a href="/Object-Detection/#SOHUCS" itemprop="discussionUrl">
                  <span id="changyan_count_unit" class="post-comments-count hc-comment-count" data-xid="Object-Detection/" itemprop="commentsCount"></span>
                </a>
              
            
          

          
          

          
            <span class="post-meta-divider">|</span>
            <span class="page-pv"><i class="fa fa-file-o"></i>&nbsp阅读量
            <span class="busuanzi-value" id="busuanzi_value_page_pv" ></span>次
            </span>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>本文转自<a href="https://handong1587.github.io/deep_learning/2015/10/09/object-detection.html" target="_blank" rel="external">handong1587的个人博客</a>，总结的很详细！辛苦了！</p>
<a id="more"></a>
<table>
<thead>
<tr>
<th style="text-align:center">Method</th>
<th style="text-align:center">backbone</th>
<th style="text-align:center">test size</th>
<th style="text-align:center">VOC2007</th>
<th style="text-align:center">VOC2010</th>
<th style="text-align:center">VOC2012</th>
<th style="text-align:center">ILSVRC 2013</th>
<th style="text-align:center">MSCOCO 2015</th>
<th style="text-align:center">Speed</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">OverFeat</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center">24.3%</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">R-CNN</td>
<td style="text-align:center">AlexNet</td>
<td style="text-align:center"></td>
<td style="text-align:center">58.5%</td>
<td style="text-align:center">53.7%</td>
<td style="text-align:center">53.3%</td>
<td style="text-align:center">31.4%</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">R-CNN</td>
<td style="text-align:center">VGG16</td>
<td style="text-align:center"></td>
<td style="text-align:center">66.0%</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">SPP_net</td>
<td style="text-align:center">ZF-5</td>
<td style="text-align:center"></td>
<td style="text-align:center">54.2%</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center">31.84%</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">DeepID-Net</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center">64.1%</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center">50.3%</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">NoC</td>
<td style="text-align:center">73.3%</td>
<td style="text-align:center"></td>
<td style="text-align:center">68.8%</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">Fast-RCNN</td>
<td style="text-align:center">VGG16</td>
<td style="text-align:center"></td>
<td style="text-align:center">70.0%</td>
<td style="text-align:center">68.8%</td>
<td style="text-align:center">68.4%</td>
<td style="text-align:center"></td>
<td style="text-align:center">19.7%(@[0.5-0.95]), 35.9%(@0.5)</td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">MR-CNN</td>
<td style="text-align:center">78.2%</td>
<td style="text-align:center"></td>
<td style="text-align:center">73.9%</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">Faster-RCNN</td>
<td style="text-align:center">VGG16</td>
<td style="text-align:center"></td>
<td style="text-align:center">78.8%</td>
<td style="text-align:center"></td>
<td style="text-align:center">75.9%</td>
<td style="text-align:center"></td>
<td style="text-align:center">21.9%(@[0.5-0.95]), 42.7%(@0.5)</td>
<td style="text-align:center">198ms</td>
</tr>
<tr>
<td style="text-align:center">Faster-RCNN</td>
<td style="text-align:center">ResNet101</td>
<td style="text-align:center"></td>
<td style="text-align:center">85.6%</td>
<td style="text-align:center"></td>
<td style="text-align:center">83.8%</td>
<td style="text-align:center"></td>
<td style="text-align:center">37.4%(@[0.5-0.95]), 59.0%(@0.5)</td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">YOLO</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center">63.4%</td>
<td style="text-align:center"></td>
<td style="text-align:center">57.9%</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center">45 fps</td>
</tr>
<tr>
<td style="text-align:center">YOLO VGG-16</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center">66.4%</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center">21 fps</td>
</tr>
<tr>
<td style="text-align:center">YOLOv2</td>
<td style="text-align:center"></td>
<td style="text-align:center">448x448</td>
<td style="text-align:center">78.6%</td>
<td style="text-align:center"></td>
<td style="text-align:center">73.4%</td>
<td style="text-align:center"></td>
<td style="text-align:center">21.6%(@[0.5-0.95]), 44.0%(@0.5)</td>
<td style="text-align:center">40 fps</td>
</tr>
<tr>
<td style="text-align:center">SSD</td>
<td style="text-align:center">VGG16</td>
<td style="text-align:center">300x300</td>
<td style="text-align:center">77.2%</td>
<td style="text-align:center"></td>
<td style="text-align:center">75.8%</td>
<td style="text-align:center"></td>
<td style="text-align:center">25.1%(@[0.5-0.95]), 43.1%(@0.5)</td>
<td style="text-align:center">46 fps</td>
</tr>
<tr>
<td style="text-align:center">SSD</td>
<td style="text-align:center">VGG16</td>
<td style="text-align:center">512x512</td>
<td style="text-align:center">79.8%</td>
<td style="text-align:center"></td>
<td style="text-align:center">78.5%</td>
<td style="text-align:center"></td>
<td style="text-align:center">28.8%(@[0.5-0.95]), 48.5%(@0.5)</td>
<td style="text-align:center">19 fps</td>
</tr>
<tr>
<td style="text-align:center">SSD</td>
<td style="text-align:center">ResNet101</td>
<td style="text-align:center">300x300</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center">28.0%(@[0.5-0.95])</td>
<td style="text-align:center">16 fps</td>
</tr>
<tr>
<td style="text-align:center">SSD</td>
<td style="text-align:center">ResNet101</td>
<td style="text-align:center">512x512</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center">31.2%(@[0.5-0.95])</td>
<td style="text-align:center">8 fps</td>
</tr>
<tr>
<td style="text-align:center">DSSD</td>
<td style="text-align:center">ResNet101</td>
<td style="text-align:center">300x300</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center">28.0%(@[0.5-0.95])</td>
<td style="text-align:center">8 fps</td>
</tr>
<tr>
<td style="text-align:center">DSSD</td>
<td style="text-align:center">ResNet101</td>
<td style="text-align:center">500x500</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center">33.2%(@[0.5-0.95])</td>
<td style="text-align:center">6 fps</td>
</tr>
<tr>
<td style="text-align:center">ION</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center">79.2%</td>
<td style="text-align:center"></td>
<td style="text-align:center">76.4%</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">CRAFT</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center">75.7%</td>
<td style="text-align:center"></td>
<td style="text-align:center">71.3%</td>
<td style="text-align:center">48.5%</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">OHEM</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center">78.9%</td>
<td style="text-align:center"></td>
<td style="text-align:center">76.3%</td>
<td style="text-align:center"></td>
<td style="text-align:center">25.5%(@[0.5-0.95]), 45.9%(@0.5)</td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">R-FCN</td>
<td style="text-align:center">ResNet50</td>
<td style="text-align:center"></td>
<td style="text-align:center">77.4%</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center">0.12sec(K40), 0.09sec(TitianX)</td>
</tr>
<tr>
<td style="text-align:center">R-FCN</td>
<td style="text-align:center">ResNet101</td>
<td style="text-align:center"></td>
<td style="text-align:center">79.5%</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center">0.17sec(K40), 0.12sec(TitianX)</td>
</tr>
<tr>
<td style="text-align:center">R-FCN(ms train)</td>
<td style="text-align:center">ResNet101</td>
<td style="text-align:center"></td>
<td style="text-align:center">83.6%</td>
<td style="text-align:center"></td>
<td style="text-align:center">82.0%</td>
<td style="text-align:center"></td>
<td style="text-align:center">31.5%(@[0.5-0.95]), 53.2%(@0.5)</td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">PVANet 9.0</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center">84.9%</td>
<td style="text-align:center"></td>
<td style="text-align:center">84.2%</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center">750ms(CPU), 46ms(TitianX)</td>
</tr>
<tr>
<td style="text-align:center">RetinaNet</td>
<td style="text-align:center">ResNet101-FPN</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">Light-Head R-CNN</td>
<td style="text-align:center">Xception*</td>
<td style="text-align:center">800/1200</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center">31.5%@[0.5:0.95]</td>
<td style="text-align:center">95 fps</td>
</tr>
<tr>
<td style="text-align:center">Light-Head R-CNN</td>
<td style="text-align:center">Xception*</td>
<td style="text-align:center">700/1100</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center">30.7%@[0.5:0.95]</td>
<td style="text-align:center">102 fps</td>
</tr>
</tbody>
</table>
<h1 id="Papers"><a href="#Papers" class="headerlink" title="Papers"></a>Papers</h1><p><strong>Deep Neural Networks for Object Detection</strong></p>
<ul>
<li>paper: <a href="http://papers.nips.cc/paper/5207-deep-neural-networks-for-object-detection.pdf" target="_blank" rel="external">http://papers.nips.cc/paper/5207-deep-neural-networks-for-object-detection.pdf</a></li>
</ul>
<p><strong>OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks</strong></p>
<ul>
<li>arxiv: <a href="http://arxiv.org/abs/1312.6229" target="_blank" rel="external">http://arxiv.org/abs/1312.6229</a></li>
<li>github: <a href="https://github.com/sermanet/OverFeat" target="_blank" rel="external">https://github.com/sermanet/OverFeat</a></li>
<li>code: <a href="http://cilvr.nyu.edu/doku.php?id=software:overfeat:start" target="_blank" rel="external">http://cilvr.nyu.edu/doku.php?id=software:overfeat:start</a></li>
</ul>
<h2 id="R-CNN"><a href="#R-CNN" class="headerlink" title="R-CNN"></a>R-CNN</h2><p><strong>Rich feature hierarchies for accurate object detection and semantic segmentation</strong></p>
<ul>
<li>intro: R-CNN</li>
<li>arxiv: <a href="http://arxiv.org/abs/1311.2524" target="_blank" rel="external">http://arxiv.org/abs/1311.2524</a></li>
<li>supp: <a href="http://people.eecs.berkeley.edu/~rbg/papers/r-cnn-cvpr-supp.pdf" target="_blank" rel="external">http://people.eecs.berkeley.edu/~rbg/papers/r-cnn-cvpr-supp.pdf</a></li>
<li>slides: <a href="http://www.image-net.org/challenges/LSVRC/2013/slides/r-cnn-ilsvrc2013-workshop.pdf" target="_blank" rel="external">http://www.image-net.org/challenges/LSVRC/2013/slides/r-cnn-ilsvrc2013-workshop.pdf</a></li>
<li>slides: <a href="http://www.cs.berkeley.edu/~rbg/slides/rcnn-cvpr14-slides.pdf" target="_blank" rel="external">http://www.cs.berkeley.edu/~rbg/slides/rcnn-cvpr14-slides.pdf</a></li>
<li>github: <a href="https://github.com/rbgirshick/rcnn" target="_blank" rel="external">https://github.com/rbgirshick/rcnn</a></li>
<li>notes: <a href="http://zhangliliang.com/2014/07/23/paper-note-rcnn/" target="_blank" rel="external">http://zhangliliang.com/2014/07/23/paper-note-rcnn/</a></li>
<li>caffe-pr(“Make R-CNN the Caffe detection example”): <a href="https://github.com/BVLC/caffe/pull/482" target="_blank" rel="external">https://github.com/BVLC/caffe/pull/482</a> </li>
</ul>
<h2 id="Fast-R-CNN"><a href="#Fast-R-CNN" class="headerlink" title="Fast R-CNN"></a>Fast R-CNN</h2><p><strong>Fast R-CNN</strong></p>
<ul>
<li>arxiv: <a href="http://arxiv.org/abs/1504.08083" target="_blank" rel="external">http://arxiv.org/abs/1504.08083</a></li>
<li>slides: <a href="http://tutorial.caffe.berkeleyvision.org/caffe-cvpr15-detection.pdf" target="_blank" rel="external">http://tutorial.caffe.berkeleyvision.org/caffe-cvpr15-detection.pdf</a></li>
<li>github: <a href="https://github.com/rbgirshick/fast-rcnn" target="_blank" rel="external">https://github.com/rbgirshick/fast-rcnn</a></li>
<li>github(COCO-branch): <a href="https://github.com/rbgirshick/fast-rcnn/tree/coco" target="_blank" rel="external">https://github.com/rbgirshick/fast-rcnn/tree/coco</a></li>
<li>webcam demo: <a href="https://github.com/rbgirshick/fast-rcnn/pull/29" target="_blank" rel="external">https://github.com/rbgirshick/fast-rcnn/pull/29</a></li>
<li>notes: <a href="http://zhangliliang.com/2015/05/17/paper-note-fast-rcnn/" target="_blank" rel="external">http://zhangliliang.com/2015/05/17/paper-note-fast-rcnn/</a></li>
<li>notes: <a href="http://blog.csdn.net/linj_m/article/details/48930179" target="_blank" rel="external">http://blog.csdn.net/linj_m/article/details/48930179</a></li>
<li>github(“Fast R-CNN in MXNet”): <a href="https://github.com/precedenceguo/mx-rcnn" target="_blank" rel="external">https://github.com/precedenceguo/mx-rcnn</a></li>
<li>github: <a href="https://github.com/mahyarnajibi/fast-rcnn-torch" target="_blank" rel="external">https://github.com/mahyarnajibi/fast-rcnn-torch</a></li>
<li>github: <a href="https://github.com/apple2373/chainer-simple-fast-rnn" target="_blank" rel="external">https://github.com/apple2373/chainer-simple-fast-rnn</a></li>
<li>github: <a href="https://github.com/zplizzi/tensorflow-fast-rcnn" target="_blank" rel="external">https://github.com/zplizzi/tensorflow-fast-rcnn</a></li>
</ul>
<p><strong>A-Fast-RCNN: Hard Positive Generation via Adversary for Object Detection</strong></p>
<ul>
<li>intro: CVPR 2017</li>
<li>arxiv: <a href="https://arxiv.org/abs/1704.03414" target="_blank" rel="external">https://arxiv.org/abs/1704.03414</a></li>
<li>paper: <a href="http://abhinavsh.info/papers/pdfs/adversarial_object_detection.pdf" target="_blank" rel="external">http://abhinavsh.info/papers/pdfs/adversarial_object_detection.pdf</a></li>
<li>github(Caffe): <a href="https://github.com/xiaolonw/adversarial-frcnn" target="_blank" rel="external">https://github.com/xiaolonw/adversarial-frcnn</a></li>
</ul>
<h2 id="Faster-R-CNN"><a href="#Faster-R-CNN" class="headerlink" title="Faster R-CNN"></a>Faster R-CNN</h2><p><strong>Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</strong></p>
<ul>
<li>intro: NIPS 2015</li>
<li>arxiv: <a href="http://arxiv.org/abs/1506.01497" target="_blank" rel="external">http://arxiv.org/abs/1506.01497</a></li>
<li>gitxiv: <a href="http://www.gitxiv.com/posts/8pfpcvefDYn2gSgXk/faster-r-cnn-towards-real-time-object-detection-with-region" target="_blank" rel="external">http://www.gitxiv.com/posts/8pfpcvefDYn2gSgXk/faster-r-cnn-towards-real-time-object-detection-with-region</a></li>
<li>slides: <a href="http://web.cs.hacettepe.edu.tr/~aykut/classes/spring2016/bil722/slides/w05-FasterR-CNN.pdf" target="_blank" rel="external">http://web.cs.hacettepe.edu.tr/~aykut/classes/spring2016/bil722/slides/w05-FasterR-CNN.pdf</a></li>
<li>github(official, Matlab): <a href="https://github.com/ShaoqingRen/faster_rcnn" target="_blank" rel="external">https://github.com/ShaoqingRen/faster_rcnn</a></li>
<li>github: <a href="https://github.com/rbgirshick/py-faster-rcnn" target="_blank" rel="external">https://github.com/rbgirshick/py-faster-rcnn</a></li>
<li>github(MXNet): <a href="https://github.com/msracver/Deformable-ConvNets/tree/master/faster_rcnn" target="_blank" rel="external">https://github.com/msracver/Deformable-ConvNets/tree/master/faster_rcnn</a></li>
<li>github: <a href="https://github.com//jwyang/faster-rcnn.pytorch" target="_blank" rel="external">https://github.com//jwyang/faster-rcnn.pytorch</a></li>
<li>github: <a href="https://github.com/mitmul/chainer-faster-rcnn" target="_blank" rel="external">https://github.com/mitmul/chainer-faster-rcnn</a></li>
<li>github: <a href="https://github.com/andreaskoepf/faster-rcnn.torch" target="_blank" rel="external">https://github.com/andreaskoepf/faster-rcnn.torch</a></li>
<li>github: <a href="https://github.com/ruotianluo/Faster-RCNN-Densecap-torch" target="_blank" rel="external">https://github.com/ruotianluo/Faster-RCNN-Densecap-torch</a></li>
<li>github: <a href="https://github.com/smallcorgi/Faster-RCNN_TF" target="_blank" rel="external">https://github.com/smallcorgi/Faster-RCNN_TF</a></li>
<li>github: <a href="https://github.com/CharlesShang/TFFRCNN" target="_blank" rel="external">https://github.com/CharlesShang/TFFRCNN</a></li>
<li>github(C++ demo): <a href="https://github.com/YihangLou/FasterRCNN-Encapsulation-Cplusplus" target="_blank" rel="external">https://github.com/YihangLou/FasterRCNN-Encapsulation-Cplusplus</a></li>
<li>github: <a href="https://github.com/yhenon/keras-frcnn" target="_blank" rel="external">https://github.com/yhenon/keras-frcnn</a></li>
<li>github: <a href="https://github.com/Eniac-Xie/faster-rcnn-resnet" target="_blank" rel="external">https://github.com/Eniac-Xie/faster-rcnn-resnet</a></li>
<li>github(C++): <a href="https://github.com/D-X-Y/caffe-faster-rcnn/tree/dev" target="_blank" rel="external">https://github.com/D-X-Y/caffe-faster-rcnn/tree/dev</a></li>
</ul>
<p><strong>R-CNN minus R</strong></p>
<ul>
<li>intro: BMVC 2015</li>
<li>arxiv: <a href="http://arxiv.org/abs/1506.06981" target="_blank" rel="external">http://arxiv.org/abs/1506.06981</a></li>
</ul>
<p><strong>Faster R-CNN in MXNet with distributed implementation and data parallelization</strong></p>
<ul>
<li>github: <a href="https://github.com/dmlc/mxnet/tree/master/example/rcnn" target="_blank" rel="external">https://github.com/dmlc/mxnet/tree/master/example/rcnn</a></li>
</ul>
<p><strong>Contextual Priming and Feedback for Faster R-CNN</strong></p>
<ul>
<li>intro: ECCV 2016. Carnegie Mellon University</li>
<li>paper: <a href="http://abhinavsh.info/context_priming_feedback.pdf" target="_blank" rel="external">http://abhinavsh.info/context_priming_feedback.pdf</a></li>
<li>poster: <a href="http://www.eccv2016.org/files/posters/P-1A-20.pdf" target="_blank" rel="external">http://www.eccv2016.org/files/posters/P-1A-20.pdf</a></li>
</ul>
<p><strong>An Implementation of Faster RCNN with Study for Region Sampling</strong></p>
<ul>
<li>intro: Technical Report, 3 pages. CMU</li>
<li>arxiv: <a href="https://arxiv.org/abs/1702.02138" target="_blank" rel="external">https://arxiv.org/abs/1702.02138</a></li>
<li>github: <a href="https://github.com/endernewton/tf-faster-rcnn" target="_blank" rel="external">https://github.com/endernewton/tf-faster-rcnn</a></li>
</ul>
<p><strong>Interpretable R-CNN</strong></p>
<ul>
<li>intro: North Carolina State University &amp; Alibaba</li>
<li>keywords: AND-OR Graph (AOG)</li>
<li>arxiv: <a href="https://arxiv.org/abs/1711.05226" target="_blank" rel="external">https://arxiv.org/abs/1711.05226</a></li>
</ul>
<h2 id="Light-Head-R-CNN"><a href="#Light-Head-R-CNN" class="headerlink" title="Light-Head R-CNN"></a>Light-Head R-CNN</h2><p><strong>Light-Head R-CNN: In Defense of Two-Stage Object Detector</strong></p>
<ul>
<li>intro: Tsinghua University &amp; Megvii Inc</li>
<li>arxiv: <a href="https://arxiv.org/abs/1711.07264" target="_blank" rel="external">https://arxiv.org/abs/1711.07264</a></li>
<li>github: <a href="https://github.com/terrychenism/Deformable-ConvNets/blob/master/rfcn/symbols/resnet_v1_101_rfcn_light.py#L784" target="_blank" rel="external">https://github.com/terrychenism/Deformable-ConvNets/blob/master/rfcn/symbols/resnet_v1_101_rfcn_light.py#L784</a></li>
</ul>
<h2 id="Cascade-R-CNN"><a href="#Cascade-R-CNN" class="headerlink" title="Cascade R-CNN"></a>Cascade R-CNN</h2><p><strong>Cascade R-CNN: Delving into High Quality Object Detection</strong></p>
<ul>
<li>arxiv: <a href="https://arxiv.org/abs/1712.00726" target="_blank" rel="external">https://arxiv.org/abs/1712.00726</a></li>
<li>github: <a href="https://github.com/zhaoweicai/cascade-rcnn" target="_blank" rel="external">https://github.com/zhaoweicai/cascade-rcnn</a></li>
</ul>
<h2 id="MultiBox"><a href="#MultiBox" class="headerlink" title="MultiBox"></a>MultiBox</h2><p><strong>Scalable Object Detection using Deep Neural Networks</strong></p>
<ul>
<li>intro: first MultiBox. Train a CNN to predict Region of Interest.</li>
<li>arxiv: <a href="http://arxiv.org/abs/1312.2249" target="_blank" rel="external">http://arxiv.org/abs/1312.2249</a></li>
<li>github: <a href="https://github.com/google/multibox" target="_blank" rel="external">https://github.com/google/multibox</a></li>
<li>blog: <a href="https://research.googleblog.com/2014/12/high-quality-object-detection-at-scale.html" target="_blank" rel="external">https://research.googleblog.com/2014/12/high-quality-object-detection-at-scale.html</a></li>
</ul>
<p><strong>Scalable, High-Quality Object Detection</strong></p>
<ul>
<li>intro: second MultiBox</li>
<li>arxiv: <a href="http://arxiv.org/abs/1412.1441" target="_blank" rel="external">http://arxiv.org/abs/1412.1441</a></li>
<li>github: <a href="https://github.com/google/multibox" target="_blank" rel="external">https://github.com/google/multibox</a></li>
</ul>
<h2 id="SPP-Net"><a href="#SPP-Net" class="headerlink" title="SPP-Net"></a>SPP-Net</h2><p><strong>Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition</strong></p>
<ul>
<li>intro: ECCV 2014 / TPAMI 2015</li>
<li>arxiv: <a href="http://arxiv.org/abs/1406.4729" target="_blank" rel="external">http://arxiv.org/abs/1406.4729</a></li>
<li>github: <a href="https://github.com/ShaoqingRen/SPP_net" target="_blank" rel="external">https://github.com/ShaoqingRen/SPP_net</a></li>
<li>notes: <a href="http://zhangliliang.com/2014/09/13/paper-note-sppnet/" target="_blank" rel="external">http://zhangliliang.com/2014/09/13/paper-note-sppnet/</a></li>
</ul>
<p><strong>DeepID-Net: Deformable Deep Convolutional Neural Networks for Object Detection</strong></p>
<ul>
<li>intro: PAMI 2016</li>
<li>intro: an extension of R-CNN. box pre-training, cascade on region proposals, deformation layers and context representations</li>
<li>project page: <a href="http://www.ee.cuhk.edu.hk/%CB%9Cwlouyang/projects/imagenetDeepId/index.html" target="_blank" rel="external">http://www.ee.cuhk.edu.hk/%CB%9Cwlouyang/projects/imagenetDeepId/index.html</a></li>
<li>arxiv: <a href="http://arxiv.org/abs/1412.5661" target="_blank" rel="external">http://arxiv.org/abs/1412.5661</a></li>
</ul>
<p><strong>Object Detectors Emerge in Deep Scene CNNs</strong></p>
<ul>
<li>intro: ICLR 2015</li>
<li>arxiv: <a href="http://arxiv.org/abs/1412.6856" target="_blank" rel="external">http://arxiv.org/abs/1412.6856</a></li>
<li>paper: <a href="https://www.robots.ox.ac.uk/~vgg/rg/papers/zhou_iclr15.pdf" target="_blank" rel="external">https://www.robots.ox.ac.uk/~vgg/rg/papers/zhou_iclr15.pdf</a></li>
<li>paper: <a href="https://people.csail.mit.edu/khosla/papers/iclr2015_zhou.pdf" target="_blank" rel="external">https://people.csail.mit.edu/khosla/papers/iclr2015_zhou.pdf</a></li>
<li>slides: <a href="http://places.csail.mit.edu/slide_iclr2015.pdf" target="_blank" rel="external">http://places.csail.mit.edu/slide_iclr2015.pdf</a></li>
</ul>
<p><strong>segDeepM: Exploiting Segmentation and Context in Deep Neural Networks for Object Detection</strong></p>
<ul>
<li>intro: CVPR 2015</li>
<li>project(code+data): <a href="https://www.cs.toronto.edu/~yukun/segdeepm.html" target="_blank" rel="external">https://www.cs.toronto.edu/~yukun/segdeepm.html</a></li>
<li>arxiv: <a href="https://arxiv.org/abs/1502.04275" target="_blank" rel="external">https://arxiv.org/abs/1502.04275</a></li>
<li>github: <a href="https://github.com/YknZhu/segDeepM" target="_blank" rel="external">https://github.com/YknZhu/segDeepM</a></li>
</ul>
<p><strong>Object Detection Networks on Convolutional Feature Maps</strong></p>
<ul>
<li>intro: TPAMI 2015</li>
<li>keywords: NoC</li>
<li>arxiv: <a href="http://arxiv.org/abs/1504.06066" target="_blank" rel="external">http://arxiv.org/abs/1504.06066</a></li>
</ul>
<p><strong>Improving Object Detection with Deep Convolutional Networks via Bayesian Optimization and Structured Prediction</strong></p>
<ul>
<li>arxiv: <a href="http://arxiv.org/abs/1504.03293" target="_blank" rel="external">http://arxiv.org/abs/1504.03293</a></li>
<li>slides: <a href="http://www.ytzhang.net/files/publications/2015-cvpr-det-slides.pdf" target="_blank" rel="external">http://www.ytzhang.net/files/publications/2015-cvpr-det-slides.pdf</a></li>
<li>github: <a href="https://github.com/YutingZhang/fgs-obj" target="_blank" rel="external">https://github.com/YutingZhang/fgs-obj</a></li>
</ul>
<p><strong>DeepBox: Learning Objectness with Convolutional Networks</strong></p>
<ul>
<li>keywords: DeepBox</li>
<li>arxiv: <a href="http://arxiv.org/abs/1505.02146" target="_blank" rel="external">http://arxiv.org/abs/1505.02146</a></li>
<li>github: <a href="https://github.com/weichengkuo/DeepBox" target="_blank" rel="external">https://github.com/weichengkuo/DeepBox</a></li>
</ul>
<h2 id="MR-CNN"><a href="#MR-CNN" class="headerlink" title="MR-CNN"></a>MR-CNN</h2><p><strong>Object detection via a multi-region &amp; semantic segmentation-aware CNN model</strong></p>
<ul>
<li>intro: ICCV 2015. MR-CNN</li>
<li>arxiv: <a href="http://arxiv.org/abs/1505.01749" target="_blank" rel="external">http://arxiv.org/abs/1505.01749</a></li>
<li>github: <a href="https://github.com/gidariss/mrcnn-object-detection" target="_blank" rel="external">https://github.com/gidariss/mrcnn-object-detection</a></li>
<li>notes: <a href="http://zhangliliang.com/2015/05/17/paper-note-ms-cnn/" target="_blank" rel="external">http://zhangliliang.com/2015/05/17/paper-note-ms-cnn/</a></li>
<li>notes: <a href="http://blog.cvmarcher.com/posts/2015/05/17/multi-region-semantic-segmentation-aware-cnn/" target="_blank" rel="external">http://blog.cvmarcher.com/posts/2015/05/17/multi-region-semantic-segmentation-aware-cnn/</a></li>
</ul>
<h2 id="YOLO"><a href="#YOLO" class="headerlink" title="YOLO"></a>YOLO</h2><p><strong>You Only Look Once: Unified, Real-Time Object Detection</strong></p>
<p><img src="https://camo.githubusercontent.com/e69d4118b20a42de4e23b9549f9a6ec6dbbb0814/687474703a2f2f706a7265646469652e636f6d2f6d656469612f66696c65732f6461726b6e65742d626c61636b2d736d616c6c2e706e67" alt=""></p>
<ul>
<li>arxiv: <a href="http://arxiv.org/abs/1506.02640" target="_blank" rel="external">http://arxiv.org/abs/1506.02640</a></li>
<li>code: <a href="http://pjreddie.com/darknet/yolo/" target="_blank" rel="external">http://pjreddie.com/darknet/yolo/</a></li>
<li>github: <a href="https://github.com/pjreddie/darknet" target="_blank" rel="external">https://github.com/pjreddie/darknet</a></li>
<li>blog: <a href="https://pjreddie.com/publications/yolo/" target="_blank" rel="external">https://pjreddie.com/publications/yolo/</a></li>
<li>slides: <a href="https://docs.google.com/presentation/d/1aeRvtKG21KHdD5lg6Hgyhx5rPq_ZOsGjG5rJ1HP7BbA/pub?start=false&amp;loop=false&amp;delayms=3000&amp;slide=id.p" target="_blank" rel="external">https://docs.google.com/presentation/d/1aeRvtKG21KHdD5lg6Hgyhx5rPq_ZOsGjG5rJ1HP7BbA/pub?start=false&amp;loop=false&amp;delayms=3000&amp;slide=id.p</a></li>
<li>reddit: <a href="https://www.reddit.com/r/MachineLearning/comments/3a3m0o/realtime_object_detection_with_yolo/" target="_blank" rel="external">https://www.reddit.com/r/MachineLearning/comments/3a3m0o/realtime_object_detection_with_yolo/</a></li>
<li>github: <a href="https://github.com/gliese581gg/YOLO_tensorflow" target="_blank" rel="external">https://github.com/gliese581gg/YOLO_tensorflow</a></li>
<li>github: <a href="https://github.com/xingwangsfu/caffe-yolo" target="_blank" rel="external">https://github.com/xingwangsfu/caffe-yolo</a></li>
<li>github: <a href="https://github.com/frankzhangrui/Darknet-Yolo" target="_blank" rel="external">https://github.com/frankzhangrui/Darknet-Yolo</a></li>
<li>github: <a href="https://github.com/BriSkyHekun/py-darknet-yolo" target="_blank" rel="external">https://github.com/BriSkyHekun/py-darknet-yolo</a></li>
<li>github: <a href="https://github.com/tommy-qichang/yolo.torch" target="_blank" rel="external">https://github.com/tommy-qichang/yolo.torch</a></li>
<li>github: <a href="https://github.com/frischzenger/yolo-windows" target="_blank" rel="external">https://github.com/frischzenger/yolo-windows</a></li>
<li>github: <a href="https://github.com/AlexeyAB/yolo-windows" target="_blank" rel="external">https://github.com/AlexeyAB/yolo-windows</a></li>
<li>github: <a href="https://github.com/nilboy/tensorflow-yolo" target="_blank" rel="external">https://github.com/nilboy/tensorflow-yolo</a></li>
</ul>
<p><strong>darkflow - translate darknet to tensorflow. Load trained weights, retrain/fine-tune them using tensorflow, export constant graph def to C++</strong></p>
<ul>
<li>blog: <a href="https://thtrieu.github.io/notes/yolo-tensorflow-graph-buffer-cpp" target="_blank" rel="external">https://thtrieu.github.io/notes/yolo-tensorflow-graph-buffer-cpp</a></li>
<li>github: <a href="https://github.com/thtrieu/darkflow" target="_blank" rel="external">https://github.com/thtrieu/darkflow</a></li>
</ul>
<p><strong>Start Training YOLO with Our Own Data</strong></p>
<p><img src="http://guanghan.info/blog/en/wp-content/uploads/2015/12/images-40.jpg" alt=""></p>
<ul>
<li>intro: train with customized data and class numbers/labels. Linux / Windows version for darknet.</li>
<li>blog: <a href="http://guanghan.info/blog/en/my-works/train-yolo/" target="_blank" rel="external">http://guanghan.info/blog/en/my-works/train-yolo/</a></li>
<li>github: <a href="https://github.com/Guanghan/darknet" target="_blank" rel="external">https://github.com/Guanghan/darknet</a></li>
</ul>
<p><strong>YOLO: Core ML versus MPSNNGraph</strong></p>
<ul>
<li>intro: Tiny YOLO for iOS implemented using CoreML but also using the new MPS graph API.</li>
<li>blog: <a href="http://machinethink.net/blog/yolo-coreml-versus-mps-graph/" target="_blank" rel="external">http://machinethink.net/blog/yolo-coreml-versus-mps-graph/</a></li>
<li>github: <a href="https://github.com/hollance/YOLO-CoreML-MPSNNGraph" target="_blank" rel="external">https://github.com/hollance/YOLO-CoreML-MPSNNGraph</a></li>
</ul>
<p><strong>TensorFlow YOLO object detection on Android</strong></p>
<ul>
<li>intro: Real-time object detection on Android using the YOLO network with TensorFlow</li>
<li>github: <a href="https://github.com/natanielruiz/android-yolo" target="_blank" rel="external">https://github.com/natanielruiz/android-yolo</a></li>
</ul>
<p><strong>Computer Vision in iOS – Object Detection</strong></p>
<ul>
<li>blog: <a href="https://sriraghu.com/2017/07/12/computer-vision-in-ios-object-detection/" target="_blank" rel="external">https://sriraghu.com/2017/07/12/computer-vision-in-ios-object-detection/</a></li>
<li>github:<a href="https://github.com/r4ghu/iOS-CoreML-Yolo" target="_blank" rel="external">https://github.com/r4ghu/iOS-CoreML-Yolo</a></li>
</ul>
<h2 id="YOLOv2"><a href="#YOLOv2" class="headerlink" title="YOLOv2"></a>YOLOv2</h2><p><strong>YOLO9000: Better, Faster, Stronger</strong></p>
<ul>
<li>arxiv: <a href="https://arxiv.org/abs/1612.08242" target="_blank" rel="external">https://arxiv.org/abs/1612.08242</a></li>
<li>code: <a href="http://pjreddie.com/yolo9000/" target="_blank" rel="external">http://pjreddie.com/yolo9000/</a></li>
<li>github(Chainer): <a href="https://github.com/leetenki/YOLOv2" target="_blank" rel="external">https://github.com/leetenki/YOLOv2</a></li>
<li>github(Keras): <a href="https://github.com/allanzelener/YAD2K" target="_blank" rel="external">https://github.com/allanzelener/YAD2K</a></li>
<li>github(PyTorch): <a href="https://github.com/longcw/yolo2-pytorch" target="_blank" rel="external">https://github.com/longcw/yolo2-pytorch</a></li>
<li>github(Tensorflow): <a href="https://github.com/hizhangp/yolo_tensorflow" target="_blank" rel="external">https://github.com/hizhangp/yolo_tensorflow</a></li>
<li>github(Windows): <a href="https://github.com/AlexeyAB/darknet" target="_blank" rel="external">https://github.com/AlexeyAB/darknet</a></li>
<li>github: <a href="https://github.com/choasUp/caffe-yolo9000" target="_blank" rel="external">https://github.com/choasUp/caffe-yolo9000</a></li>
<li>github: <a href="https://github.com/philipperemy/yolo-9000" target="_blank" rel="external">https://github.com/philipperemy/yolo-9000</a></li>
</ul>
<p><strong>darknet_scripts</strong></p>
<ul>
<li>intro: Auxilary scripts to work with (YOLO) darknet deep learning famework. AKA -&gt; How to generate YOLO anchors?</li>
<li>github: <a href="https://github.com/Jumabek/darknet_scripts" target="_blank" rel="external">https://github.com/Jumabek/darknet_scripts</a></li>
</ul>
<p><strong>Yolo_mark: GUI for marking bounded boxes of objects in images for training Yolo v2</strong></p>
<ul>
<li>github: <a href="https://github.com/AlexeyAB/Yolo_mark" target="_blank" rel="external">https://github.com/AlexeyAB/Yolo_mark</a></li>
</ul>
<p><strong>LightNet: Bringing pjreddie’s DarkNet out of the shadows</strong></p>
<p><a href="https://github.com//explosion/lightnet" target="_blank" rel="external">https://github.com//explosion/lightnet</a></p>
<p><strong>YOLO v2 Bounding Box Tool</strong></p>
<ul>
<li>intro: Bounding box labeler tool to generate the training data in the format YOLO v2 requires.</li>
<li>github: <a href="https://github.com/Cartucho/yolo-boundingbox-labeler-GUI" target="_blank" rel="external">https://github.com/Cartucho/yolo-boundingbox-labeler-GUI</a></li>
</ul>
<h1 id="YOLOv3"><a href="#YOLOv3" class="headerlink" title="YOLOv3"></a>YOLOv3</h1><p><strong>YOLOv3: An Incremental Improvement</strong></p>
<ul>
<li>project page: <a href="https://pjreddie.com/darknet/yolo/" target="_blank" rel="external">https://pjreddie.com/darknet/yolo/</a></li>
<li>paper: <a href="https://pjreddie.com/media/files/papers/YOLOv3.pdf" target="_blank" rel="external">https://pjreddie.com/media/files/papers/YOLOv3.pdf</a></li>
</ul>
<hr>
<p><strong>AttentionNet: Aggregating Weak Directions for Accurate Object Detection</strong></p>
<ul>
<li>intro: ICCV 2015</li>
<li>intro: state-of-the-art performance of 65% (AP) on PASCAL VOC 2007/2012 human detection task</li>
<li>arxiv: <a href="http://arxiv.org/abs/1506.07704" target="_blank" rel="external">http://arxiv.org/abs/1506.07704</a></li>
<li>slides: <a href="https://www.robots.ox.ac.uk/~vgg/rg/slides/AttentionNet.pdf" target="_blank" rel="external">https://www.robots.ox.ac.uk/~vgg/rg/slides/AttentionNet.pdf</a></li>
<li>slides: <a href="http://image-net.org/challenges/talks/lunit-kaist-slide.pdf" target="_blank" rel="external">http://image-net.org/challenges/talks/lunit-kaist-slide.pdf</a></li>
</ul>
<h2 id="DenseBox"><a href="#DenseBox" class="headerlink" title="DenseBox"></a>DenseBox</h2><p><strong>DenseBox: Unifying Landmark Localization with End to End Object Detection</strong></p>
<ul>
<li>arxiv: <a href="http://arxiv.org/abs/1509.04874" target="_blank" rel="external">http://arxiv.org/abs/1509.04874</a></li>
<li>demo: <a href="http://pan.baidu.com/s/1mgoWWsS" target="_blank" rel="external">http://pan.baidu.com/s/1mgoWWsS</a></li>
<li>KITTI result: <a href="http://www.cvlibs.net/datasets/kitti/eval_object.php" target="_blank" rel="external">http://www.cvlibs.net/datasets/kitti/eval_object.php</a></li>
</ul>
<h2 id="SSD"><a href="#SSD" class="headerlink" title="SSD"></a>SSD</h2><p><strong>SSD: Single Shot MultiBox Detector</strong></p>
<p><img src="https://camo.githubusercontent.com/ad9b147ed3a5f48ffb7c3540711c15aa04ce49c6/687474703a2f2f7777772e63732e756e632e6564752f7e776c69752f7061706572732f7373642e706e67" alt=""></p>
<ul>
<li>intro: ECCV 2016 Oral</li>
<li>arxiv: <a href="http://arxiv.org/abs/1512.02325" target="_blank" rel="external">http://arxiv.org/abs/1512.02325</a></li>
<li>paper: <a href="http://www.cs.unc.edu/~wliu/papers/ssd.pdf" target="_blank" rel="external">http://www.cs.unc.edu/~wliu/papers/ssd.pdf</a></li>
<li>slides: <a href="http://www.cs.unc.edu/%7Ewliu/papers/ssd_eccv2016_slide.pdf" target="_blank" rel="external">http://www.cs.unc.edu/%7Ewliu/papers/ssd_eccv2016_slide.pdf</a></li>
<li>github(Official): <a href="https://github.com/weiliu89/caffe/tree/ssd" target="_blank" rel="external">https://github.com/weiliu89/caffe/tree/ssd</a></li>
<li>video: <a href="http://weibo.com/p/2304447a2326da963254c963c97fb05dd3a973" target="_blank" rel="external">http://weibo.com/p/2304447a2326da963254c963c97fb05dd3a973</a></li>
<li>github: <a href="https://github.com/zhreshold/mxnet-ssd" target="_blank" rel="external">https://github.com/zhreshold/mxnet-ssd</a></li>
<li>github: <a href="https://github.com/zhreshold/mxnet-ssd.cpp" target="_blank" rel="external">https://github.com/zhreshold/mxnet-ssd.cpp</a></li>
<li>github: <a href="https://github.com/rykov8/ssd_keras" target="_blank" rel="external">https://github.com/rykov8/ssd_keras</a></li>
<li>github: <a href="https://github.com/balancap/SSD-Tensorflow" target="_blank" rel="external">https://github.com/balancap/SSD-Tensorflow</a></li>
<li>github: <a href="https://github.com/amdegroot/ssd.pytorch" target="_blank" rel="external">https://github.com/amdegroot/ssd.pytorch</a></li>
<li>github(Caffe): <a href="https://github.com/chuanqi305/MobileNet-SSD" target="_blank" rel="external">https://github.com/chuanqi305/MobileNet-SSD</a></li>
</ul>
<p><strong>What’s the diffience in performance between this new code you pushed and the previous code? #327</strong></p>
<p><a href="https://github.com/weiliu89/caffe/issues/327" target="_blank" rel="external">https://github.com/weiliu89/caffe/issues/327</a></p>
<h2 id="DSSD"><a href="#DSSD" class="headerlink" title="DSSD"></a>DSSD</h2><p><strong>DSSD : Deconvolutional Single Shot Detector</strong></p>
<ul>
<li>intro: UNC Chapel Hill &amp; Amazon Inc</li>
<li>arxiv: <a href="https://arxiv.org/abs/1701.06659" target="_blank" rel="external">https://arxiv.org/abs/1701.06659</a></li>
<li>github: <a href="https://github.com/chengyangfu/caffe/tree/dssd" target="_blank" rel="external">https://github.com/chengyangfu/caffe/tree/dssd</a></li>
<li>github: <a href="https://github.com/MTCloudVision/mxnet-dssd" target="_blank" rel="external">https://github.com/MTCloudVision/mxnet-dssd</a></li>
<li>demo: <a href="http://120.52.72.53/www.cs.unc.edu/c3pr90ntc0td/~cyfu/dssd_lalaland.mp4" target="_blank" rel="external">http://120.52.72.53/www.cs.unc.edu/c3pr90ntc0td/~cyfu/dssd_lalaland.mp4</a></li>
</ul>
<p><strong>Enhancement of SSD by concatenating feature maps for object detection</strong></p>
<ul>
<li>intro: rainbow SSD (R-SSD)</li>
<li>arxiv: <a href="https://arxiv.org/abs/1705.09587" target="_blank" rel="external">https://arxiv.org/abs/1705.09587</a></li>
</ul>
<p><strong>Context-aware Single-Shot Detector</strong></p>
<ul>
<li>keywords: CSSD, DiCSSD, DeCSSD, effective receptive fields (ERFs),  theoretical receptive fields (TRFs)</li>
<li>arxiv: <a href="https://arxiv.org/abs/1707.08682" target="_blank" rel="external">https://arxiv.org/abs/1707.08682</a></li>
</ul>
<p><strong>Feature-Fused SSD: Fast Detection for Small Objects</strong></p>
<p><a href="https://arxiv.org/abs/1709.05054" target="_blank" rel="external">https://arxiv.org/abs/1709.05054</a></p>
<h2 id="FSSD"><a href="#FSSD" class="headerlink" title="FSSD"></a>FSSD</h2><p><strong>FSSD: Feature Fusion Single Shot Multibox Detector</strong></p>
<p><a href="https://arxiv.org/abs/1712.00960" target="_blank" rel="external">https://arxiv.org/abs/1712.00960</a></p>
<p><strong>Weaving Multi-scale Context for Single Shot Detector</strong></p>
<ul>
<li>intro: WeaveNet</li>
<li>keywords: fuse multi-scale information</li>
<li>arxiv: <a href="https://arxiv.org/abs/1712.03149" target="_blank" rel="external">https://arxiv.org/abs/1712.03149</a></li>
</ul>
<h2 id="ESSD"><a href="#ESSD" class="headerlink" title="ESSD"></a>ESSD</h2><p><strong>Extend the shallow part of Single Shot MultiBox Detector via Convolutional Neural Network</strong></p>
<p><a href="https://arxiv.org/abs/1801.05918" target="_blank" rel="external">https://arxiv.org/abs/1801.05918</a></p>
<p><strong>Tiny SSD: A Tiny Single-shot Detection Deep Convolutional Neural Network for Real-time Embedded Object Detection</strong></p>
<p><a href="https://arxiv.org/abs/1802.06488" target="_blank" rel="external">https://arxiv.org/abs/1802.06488</a></p>
<h2 id="Inside-Outside-Net-ION"><a href="#Inside-Outside-Net-ION" class="headerlink" title="Inside-Outside Net (ION)"></a>Inside-Outside Net (ION)</h2><p><strong>Inside-Outside Net: Detecting Objects in Context with Skip Pooling and Recurrent Neural Networks</strong></p>
<ul>
<li>intro: “0.8s per image on a Titan X GPU (excluding proposal generation) without two-stage bounding-box regression<br>and 1.15s per image with it”.</li>
<li>arxiv: <a href="http://arxiv.org/abs/1512.04143" target="_blank" rel="external">http://arxiv.org/abs/1512.04143</a></li>
<li>slides: <a href="http://www.seanbell.ca/tmp/ion-coco-talk-bell2015.pdf" target="_blank" rel="external">http://www.seanbell.ca/tmp/ion-coco-talk-bell2015.pdf</a></li>
<li>coco-leaderboard: <a href="http://mscoco.org/dataset/#detections-leaderboard" target="_blank" rel="external">http://mscoco.org/dataset/#detections-leaderboard</a></li>
</ul>
<p><strong>Adaptive Object Detection Using Adjacency and Zoom Prediction</strong></p>
<ul>
<li>intro: CVPR 2016. AZ-Net</li>
<li>arxiv: <a href="http://arxiv.org/abs/1512.07711" target="_blank" rel="external">http://arxiv.org/abs/1512.07711</a></li>
<li>github: <a href="https://github.com/luyongxi/az-net" target="_blank" rel="external">https://github.com/luyongxi/az-net</a></li>
<li>youtube: <a href="https://www.youtube.com/watch?v=YmFtuNwxaNM" target="_blank" rel="external">https://www.youtube.com/watch?v=YmFtuNwxaNM</a></li>
</ul>
<p><strong>G-CNN: an Iterative Grid Based Object Detector</strong></p>
<ul>
<li>arxiv: <a href="http://arxiv.org/abs/1512.07729" target="_blank" rel="external">http://arxiv.org/abs/1512.07729</a></li>
</ul>
<p><strong>Factors in Finetuning Deep Model for object detection</strong></p>
<p><strong>Factors in Finetuning Deep Model for Object Detection with Long-tail Distribution</strong></p>
<ul>
<li>intro: CVPR 2016.rank 3rd for provided data and 2nd for external data on ILSVRC 2015 object detection</li>
<li>project page: <a href="http://www.ee.cuhk.edu.hk/~wlouyang/projects/ImageNetFactors/CVPR16.html" target="_blank" rel="external">http://www.ee.cuhk.edu.hk/~wlouyang/projects/ImageNetFactors/CVPR16.html</a></li>
<li>arxiv: <a href="http://arxiv.org/abs/1601.05150" target="_blank" rel="external">http://arxiv.org/abs/1601.05150</a></li>
</ul>
<p><strong>We don’t need no bounding-boxes: Training object class detectors using only human verification</strong></p>
<ul>
<li>arxiv: <a href="http://arxiv.org/abs/1602.08405" target="_blank" rel="external">http://arxiv.org/abs/1602.08405</a></li>
</ul>
<p><strong>HyperNet: Towards Accurate Region Proposal Generation and Joint Object Detection</strong></p>
<ul>
<li>arxiv: <a href="http://arxiv.org/abs/1604.00600" target="_blank" rel="external">http://arxiv.org/abs/1604.00600</a></li>
</ul>
<p><strong>A MultiPath Network for Object Detection</strong></p>
<ul>
<li>intro: BMVC 2016. Facebook AI Research (FAIR)</li>
<li>arxiv: <a href="http://arxiv.org/abs/1604.02135" target="_blank" rel="external">http://arxiv.org/abs/1604.02135</a></li>
<li>github: <a href="https://github.com/facebookresearch/multipathnet" target="_blank" rel="external">https://github.com/facebookresearch/multipathnet</a></li>
</ul>
<h2 id="CRAFT"><a href="#CRAFT" class="headerlink" title="CRAFT"></a>CRAFT</h2><p><strong>CRAFT Objects from Images</strong></p>
<ul>
<li>intro: CVPR 2016. Cascade Region-proposal-network And FasT-rcnn. an extension of Faster R-CNN</li>
<li>project page: <a href="http://byangderek.github.io/projects/craft.html" target="_blank" rel="external">http://byangderek.github.io/projects/craft.html</a></li>
<li>arxiv: <a href="https://arxiv.org/abs/1604.03239" target="_blank" rel="external">https://arxiv.org/abs/1604.03239</a></li>
<li>paper: <a href="http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Yang_CRAFT_Objects_From_CVPR_2016_paper.pdf" target="_blank" rel="external">http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Yang_CRAFT_Objects_From_CVPR_2016_paper.pdf</a></li>
<li>github: <a href="https://github.com/byangderek/CRAFT" target="_blank" rel="external">https://github.com/byangderek/CRAFT</a></li>
</ul>
<h2 id="OHEM"><a href="#OHEM" class="headerlink" title="OHEM"></a>OHEM</h2><p><strong>Training Region-based Object Detectors with Online Hard Example Mining</strong></p>
<ul>
<li>intro: CVPR 2016 Oral. Online hard example mining (OHEM)</li>
<li>arxiv: <a href="http://arxiv.org/abs/1604.03540" target="_blank" rel="external">http://arxiv.org/abs/1604.03540</a></li>
<li>paper: <a href="http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Shrivastava_Training_Region-Based_Object_CVPR_2016_paper.pdf" target="_blank" rel="external">http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Shrivastava_Training_Region-Based_Object_CVPR_2016_paper.pdf</a></li>
<li>github(Official): <a href="https://github.com/abhi2610/ohem" target="_blank" rel="external">https://github.com/abhi2610/ohem</a></li>
<li>author page: <a href="http://abhinav-shrivastava.info/" target="_blank" rel="external">http://abhinav-shrivastava.info/</a></li>
</ul>
<p><strong>S-OHEM: Stratified Online Hard Example Mining for Object Detection</strong></p>
<p><a href="https://arxiv.org/abs/1705.02233" target="_blank" rel="external">https://arxiv.org/abs/1705.02233</a></p>
<hr>
<p><strong>Exploit All the Layers: Fast and Accurate CNN Object Detector with Scale Dependent Pooling and Cascaded Rejection Classifiers</strong></p>
<ul>
<li>intro: CVPR 2016</li>
<li>keywords: scale-dependent pooling  (SDP), cascaded rejection classifiers (CRC)</li>
<li>paper: <a href="http://www-personal.umich.edu/~wgchoi/SDP-CRC_camready.pdf" target="_blank" rel="external">http://www-personal.umich.edu/~wgchoi/SDP-CRC_camready.pdf</a></li>
</ul>
<h2 id="R-FCN"><a href="#R-FCN" class="headerlink" title="R-FCN"></a>R-FCN</h2><p><strong>R-FCN: Object Detection via Region-based Fully Convolutional Networks</strong></p>
<ul>
<li>arxiv: <a href="http://arxiv.org/abs/1605.06409" target="_blank" rel="external">http://arxiv.org/abs/1605.06409</a></li>
<li>github: <a href="https://github.com/daijifeng001/R-FCN" target="_blank" rel="external">https://github.com/daijifeng001/R-FCN</a></li>
<li>github(MXNet): <a href="https://github.com/msracver/Deformable-ConvNets/tree/master/rfcn" target="_blank" rel="external">https://github.com/msracver/Deformable-ConvNets/tree/master/rfcn</a></li>
<li>github: <a href="https://github.com/Orpine/py-R-FCN" target="_blank" rel="external">https://github.com/Orpine/py-R-FCN</a></li>
<li>github: <a href="https://github.com/PureDiors/pytorch_RFCN" target="_blank" rel="external">https://github.com/PureDiors/pytorch_RFCN</a></li>
<li>github: <a href="https://github.com/bharatsingh430/py-R-FCN-multiGPU" target="_blank" rel="external">https://github.com/bharatsingh430/py-R-FCN-multiGPU</a></li>
<li>github: <a href="https://github.com/xdever/RFCN-tensorflow" target="_blank" rel="external">https://github.com/xdever/RFCN-tensorflow</a></li>
</ul>
<p><strong>R-FCN-3000 at 30fps: Decoupling Detection and Classification</strong></p>
<p><a href="https://arxiv.org/abs/1712.01802" target="_blank" rel="external">https://arxiv.org/abs/1712.01802</a></p>
<p><strong>Recycle deep features for better object detection</strong></p>
<ul>
<li>arxiv: <a href="http://arxiv.org/abs/1607.05066" target="_blank" rel="external">http://arxiv.org/abs/1607.05066</a></li>
</ul>
<h2 id="MS-CNN"><a href="#MS-CNN" class="headerlink" title="MS-CNN"></a>MS-CNN</h2><p><strong>A Unified Multi-scale Deep Convolutional Neural Network for Fast Object Detection</strong></p>
<ul>
<li>intro: ECCV 2016</li>
<li>intro: 640×480: 15 fps, 960×720: 8 fps</li>
<li>arxiv: <a href="http://arxiv.org/abs/1607.07155" target="_blank" rel="external">http://arxiv.org/abs/1607.07155</a></li>
<li>github: <a href="https://github.com/zhaoweicai/mscnn" target="_blank" rel="external">https://github.com/zhaoweicai/mscnn</a></li>
<li>poster: <a href="http://www.eccv2016.org/files/posters/P-2B-38.pdf" target="_blank" rel="external">http://www.eccv2016.org/files/posters/P-2B-38.pdf</a></li>
</ul>
<p><strong>Multi-stage Object Detection with Group Recursive Learning</strong></p>
<ul>
<li>intro: VOC2007: 78.6%, VOC2012: 74.9%</li>
<li>arxiv: <a href="http://arxiv.org/abs/1608.05159" target="_blank" rel="external">http://arxiv.org/abs/1608.05159</a></li>
</ul>
<p><strong>Subcategory-aware Convolutional Neural Networks for Object Proposals and Detection</strong></p>
<ul>
<li>intro: WACV 2017. SubCNN</li>
<li>arxiv: <a href="http://arxiv.org/abs/1604.04693" target="_blank" rel="external">http://arxiv.org/abs/1604.04693</a></li>
<li>github: <a href="https://github.com/tanshen/SubCNN" target="_blank" rel="external">https://github.com/tanshen/SubCNN</a></li>
</ul>
<h2 id="PVANET"><a href="#PVANET" class="headerlink" title="PVANET"></a>PVANET</h2><p><strong>PVANet: Lightweight Deep Neural Networks for Real-time Object Detection</strong></p>
<ul>
<li>intro: Presented at NIPS 2016 Workshop on Efficient Methods for Deep Neural Networks (EMDNN).<br>Continuation of <a href="https://arxiv.org/abs/1608.08021" target="_blank" rel="external">arXiv:1608.08021</a></li>
<li>arxiv: <a href="https://arxiv.org/abs/1611.08588" target="_blank" rel="external">https://arxiv.org/abs/1611.08588</a></li>
<li>github: <a href="https://github.com/sanghoon/pva-faster-rcnn" target="_blank" rel="external">https://github.com/sanghoon/pva-faster-rcnn</a></li>
<li>leaderboard(PVANet 9.0): <a href="http://host.robots.ox.ac.uk:8080/leaderboard/displaylb.php?challengeid=11&amp;compid=4" target="_blank" rel="external">http://host.robots.ox.ac.uk:8080/leaderboard/displaylb.php?challengeid=11&amp;compid=4</a></li>
</ul>
<h2 id="GBD-Net"><a href="#GBD-Net" class="headerlink" title="GBD-Net"></a>GBD-Net</h2><p><strong>Gated Bi-directional CNN for Object Detection</strong></p>
<ul>
<li>intro: The Chinese University of Hong Kong &amp; Sensetime Group Limited</li>
<li>paper: <a href="http://link.springer.com/chapter/10.1007/978-3-319-46478-7_22" target="_blank" rel="external">http://link.springer.com/chapter/10.1007/978-3-319-46478-7_22</a></li>
<li>mirror: <a href="https://pan.baidu.com/s/1dFohO7v" target="_blank" rel="external">https://pan.baidu.com/s/1dFohO7v</a></li>
</ul>
<p><strong>Crafting GBD-Net for Object Detection</strong></p>
<ul>
<li>intro: winner of the ImageNet object detection challenge of 2016. CUImage and CUVideo</li>
<li>intro: gated bi-directional CNN (GBD-Net)</li>
<li>arxiv: <a href="https://arxiv.org/abs/1610.02579" target="_blank" rel="external">https://arxiv.org/abs/1610.02579</a></li>
<li>github: <a href="https://github.com/craftGBD/craftGBD" target="_blank" rel="external">https://github.com/craftGBD/craftGBD</a></li>
</ul>
<p><strong>StuffNet: Using ‘Stuff’ to Improve Object Detection</strong></p>
<ul>
<li>arxiv: <a href="https://arxiv.org/abs/1610.05861" target="_blank" rel="external">https://arxiv.org/abs/1610.05861</a></li>
</ul>
<p><strong>Generalized Haar Filter based Deep Networks for Real-Time Object Detection in Traffic Scene</strong></p>
<ul>
<li>arxiv: <a href="https://arxiv.org/abs/1610.09609" target="_blank" rel="external">https://arxiv.org/abs/1610.09609</a></li>
</ul>
<p><strong>Hierarchical Object Detection with Deep Reinforcement Learning</strong></p>
<ul>
<li>intro: Deep Reinforcement Learning Workshop (NIPS 2016)</li>
<li>project page: <a href="https://imatge-upc.github.io/detection-2016-nipsws/" target="_blank" rel="external">https://imatge-upc.github.io/detection-2016-nipsws/</a></li>
<li>arxiv: <a href="https://arxiv.org/abs/1611.03718" target="_blank" rel="external">https://arxiv.org/abs/1611.03718</a></li>
<li>slides: <a href="http://www.slideshare.net/xavigiro/hierarchical-object-detection-with-deep-reinforcement-learning" target="_blank" rel="external">http://www.slideshare.net/xavigiro/hierarchical-object-detection-with-deep-reinforcement-learning</a></li>
<li>github: <a href="https://github.com/imatge-upc/detection-2016-nipsws" target="_blank" rel="external">https://github.com/imatge-upc/detection-2016-nipsws</a></li>
<li>blog: <a href="http://jorditorres.org/nips/" target="_blank" rel="external">http://jorditorres.org/nips/</a></li>
</ul>
<p><strong>Learning to detect and localize many objects from few examples</strong></p>
<ul>
<li>arxiv: <a href="https://arxiv.org/abs/1611.05664" target="_blank" rel="external">https://arxiv.org/abs/1611.05664</a></li>
</ul>
<p><strong>Speed/accuracy trade-offs for modern convolutional object detectors</strong></p>
<ul>
<li>intro: CVPR 2017. Google Research</li>
<li>arxiv: <a href="https://arxiv.org/abs/1611.10012" target="_blank" rel="external">https://arxiv.org/abs/1611.10012</a></li>
</ul>
<p><strong>SqueezeDet: Unified, Small, Low Power Fully Convolutional Neural Networks for Real-Time Object Detection for Autonomous Driving</strong></p>
<ul>
<li>arxiv: <a href="https://arxiv.org/abs/1612.01051" target="_blank" rel="external">https://arxiv.org/abs/1612.01051</a></li>
<li>github: <a href="https://github.com/BichenWuUCB/squeezeDet" target="_blank" rel="external">https://github.com/BichenWuUCB/squeezeDet</a></li>
<li>github: <a href="https://github.com/fregu856/2D_detection" target="_blank" rel="external">https://github.com/fregu856/2D_detection</a></li>
</ul>
<h2 id="Feature-Pyramid-Network-FPN"><a href="#Feature-Pyramid-Network-FPN" class="headerlink" title="Feature Pyramid Network (FPN)"></a>Feature Pyramid Network (FPN)</h2><p><strong>Feature Pyramid Networks for Object Detection</strong></p>
<ul>
<li>intro: Facebook AI Research</li>
<li>arxiv: <a href="https://arxiv.org/abs/1612.03144" target="_blank" rel="external">https://arxiv.org/abs/1612.03144</a></li>
</ul>
<p><strong>Action-Driven Object Detection with Top-Down Visual Attentions</strong></p>
<ul>
<li>arxiv: <a href="https://arxiv.org/abs/1612.06704" target="_blank" rel="external">https://arxiv.org/abs/1612.06704</a></li>
</ul>
<p><strong>Beyond Skip Connections: Top-Down Modulation for Object Detection</strong></p>
<ul>
<li>intro: CMU &amp; UC Berkeley &amp; Google Research</li>
<li>arxiv: <a href="https://arxiv.org/abs/1612.06851" target="_blank" rel="external">https://arxiv.org/abs/1612.06851</a></li>
</ul>
<p><strong>Wide-Residual-Inception Networks for Real-time Object Detection</strong></p>
<ul>
<li>intro: Inha University</li>
<li>arxiv: <a href="https://arxiv.org/abs/1702.01243" target="_blank" rel="external">https://arxiv.org/abs/1702.01243</a></li>
</ul>
<p><strong>Attentional Network for Visual Object Detection</strong></p>
<ul>
<li>intro: University of Maryland &amp; Mitsubishi Electric Research Laboratories</li>
<li>arxiv: <a href="https://arxiv.org/abs/1702.01478" target="_blank" rel="external">https://arxiv.org/abs/1702.01478</a></li>
</ul>
<p><strong>Learning Chained Deep Features and Classifiers for Cascade in Object Detection</strong></p>
<ul>
<li>keykwords: CC-Net</li>
<li>intro: chained cascade network (CC-Net). 81.1% mAP on PASCAL VOC 2007</li>
<li>arxiv: <a href="https://arxiv.org/abs/1702.07054" target="_blank" rel="external">https://arxiv.org/abs/1702.07054</a></li>
</ul>
<p><strong>DeNet: Scalable Real-time Object Detection with Directed Sparse Sampling</strong></p>
<ul>
<li>intro: ICCV 2017 (poster)</li>
<li>arxiv: <a href="https://arxiv.org/abs/1703.10295" target="_blank" rel="external">https://arxiv.org/abs/1703.10295</a></li>
</ul>
<p><strong>Discriminative Bimodal Networks for Visual Localization and Detection with Natural Language Queries</strong></p>
<ul>
<li>intro: CVPR 2017</li>
<li>arxiv: <a href="https://arxiv.org/abs/1704.03944" target="_blank" rel="external">https://arxiv.org/abs/1704.03944</a></li>
</ul>
<p><strong>Spatial Memory for Context Reasoning in Object Detection</strong></p>
<ul>
<li>arxiv: <a href="https://arxiv.org/abs/1704.04224" target="_blank" rel="external">https://arxiv.org/abs/1704.04224</a></li>
</ul>
<p><strong>Accurate Single Stage Detector Using Recurrent Rolling Convolution</strong></p>
<ul>
<li>intro: CVPR 2017. SenseTime</li>
<li>keywords: Recurrent Rolling Convolution (RRC)</li>
<li>arxiv: <a href="https://arxiv.org/abs/1704.05776" target="_blank" rel="external">https://arxiv.org/abs/1704.05776</a></li>
<li>github: <a href="https://github.com/xiaohaoChen/rrc_detection" target="_blank" rel="external">https://github.com/xiaohaoChen/rrc_detection</a></li>
</ul>
<p><strong>Deep Occlusion Reasoning for Multi-Camera Multi-Target Detection</strong></p>
<p><a href="https://arxiv.org/abs/1704.05775" target="_blank" rel="external">https://arxiv.org/abs/1704.05775</a></p>
<p><strong>LCDet: Low-Complexity Fully-Convolutional Neural Networks for Object Detection in Embedded Systems</strong></p>
<ul>
<li>intro: Embedded Vision Workshop in CVPR. UC San Diego &amp; Qualcomm Inc</li>
<li>arxiv: <a href="https://arxiv.org/abs/1705.05922" target="_blank" rel="external">https://arxiv.org/abs/1705.05922</a></li>
</ul>
<p><strong>Point Linking Network for Object Detection</strong></p>
<ul>
<li>intro: Point Linking Network (PLN)</li>
<li>arxiv: <a href="https://arxiv.org/abs/1706.03646" target="_blank" rel="external">https://arxiv.org/abs/1706.03646</a></li>
</ul>
<p><strong>Perceptual Generative Adversarial Networks for Small Object Detection</strong></p>
<p><a href="https://arxiv.org/abs/1706.05274" target="_blank" rel="external">https://arxiv.org/abs/1706.05274</a></p>
<p><strong>Few-shot Object Detection</strong></p>
<p><a href="https://arxiv.org/abs/1706.08249" target="_blank" rel="external">https://arxiv.org/abs/1706.08249</a></p>
<p><strong>Yes-Net: An effective Detector Based on Global Information</strong></p>
<p><a href="https://arxiv.org/abs/1706.09180" target="_blank" rel="external">https://arxiv.org/abs/1706.09180</a></p>
<p><strong>SMC Faster R-CNN: Toward a scene-specialized multi-object detector</strong></p>
<p><a href="https://arxiv.org/abs/1706.10217" target="_blank" rel="external">https://arxiv.org/abs/1706.10217</a></p>
<p><strong>Towards lightweight convolutional neural networks for object detection</strong></p>
<p><a href="https://arxiv.org/abs/1707.01395" target="_blank" rel="external">https://arxiv.org/abs/1707.01395</a></p>
<p><strong>RON: Reverse Connection with Objectness Prior Networks for Object Detection</strong></p>
<ul>
<li>intro: CVPR 2017</li>
<li>arxiv: <a href="https://arxiv.org/abs/1707.01691" target="_blank" rel="external">https://arxiv.org/abs/1707.01691</a></li>
<li>github: <a href="https://github.com/taokong/RON" target="_blank" rel="external">https://github.com/taokong/RON</a></li>
</ul>
<p><strong>Mimicking Very Efficient Network for Object Detection</strong></p>
<ul>
<li>intro: CVPR 2017. SenseTime &amp; Beihang University</li>
<li>paper: <a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Li_Mimicking_Very_Efficient_CVPR_2017_paper.pdf" target="_blank" rel="external">http://openaccess.thecvf.com/content_cvpr_2017/papers/Li_Mimicking_Very_Efficient_CVPR_2017_paper.pdf</a></li>
</ul>
<p><strong>Residual Features and Unified Prediction Network for Single Stage Detection</strong></p>
<p><a href="https://arxiv.org/abs/1707.05031" target="_blank" rel="external">https://arxiv.org/abs/1707.05031</a></p>
<p><strong>Deformable Part-based Fully Convolutional Network for Object Detection</strong></p>
<ul>
<li>intro: BMVC 2017 (oral). Sorbonne Universités &amp; CEDRIC</li>
<li>arxiv: <a href="https://arxiv.org/abs/1707.06175" target="_blank" rel="external">https://arxiv.org/abs/1707.06175</a></li>
</ul>
<p><strong>Adaptive Feeding: Achieving Fast and Accurate Detections by Adaptively Combining Object Detectors</strong></p>
<ul>
<li>intro: ICCV 2017</li>
<li>arxiv: <a href="https://arxiv.org/abs/1707.06399" target="_blank" rel="external">https://arxiv.org/abs/1707.06399</a></li>
</ul>
<p><strong>Recurrent Scale Approximation for Object Detection in CNN</strong></p>
<ul>
<li>intro: ICCV 2017</li>
<li>keywords: Recurrent Scale Approximation (RSA)</li>
<li>arxiv: <a href="https://arxiv.org/abs/1707.09531" target="_blank" rel="external">https://arxiv.org/abs/1707.09531</a></li>
<li>github: <a href="https://github.com/sciencefans/RSA-for-object-detection" target="_blank" rel="external">https://github.com/sciencefans/RSA-for-object-detection</a></li>
</ul>
<h2 id="DSOD"><a href="#DSOD" class="headerlink" title="DSOD"></a>DSOD</h2><p><strong>DSOD: Learning Deeply Supervised Object Detectors from Scratch</strong></p>
<p><img src="https://user-images.githubusercontent.com/3794909/28934967-718c9302-78b5-11e7-89ee-8b514e53e23c.png" alt=""></p>
<ul>
<li>intro: ICCV 2017. Fudan University &amp; Tsinghua University &amp; Intel Labs China</li>
<li>arxiv: <a href="https://arxiv.org/abs/1708.01241" target="_blank" rel="external">https://arxiv.org/abs/1708.01241</a></li>
<li>github: <a href="https://github.com/szq0214/DSOD" target="_blank" rel="external">https://github.com/szq0214/DSOD</a></li>
</ul>
<h2 id="RetinaNet"><a href="#RetinaNet" class="headerlink" title="RetinaNet"></a>RetinaNet</h2><p><strong>Focal Loss for Dense Object Detection</strong></p>
<ul>
<li>intro: ICCV 2017 Best student paper award. Facebook AI Research</li>
<li>keywords: RetinaNet</li>
<li>arxiv: <a href="https://arxiv.org/abs/1708.02002" target="_blank" rel="external">https://arxiv.org/abs/1708.02002</a></li>
</ul>
<p><strong>CoupleNet: Coupling Global Structure with Local Parts for Object Detection</strong></p>
<ul>
<li>intro: ICCV 2017</li>
<li>arxiv: <a href="https://arxiv.org/abs/1708.02863" target="_blank" rel="external">https://arxiv.org/abs/1708.02863</a></li>
</ul>
<p><strong>Incremental Learning of Object Detectors without Catastrophic Forgetting</strong></p>
<ul>
<li>intro: ICCV 2017. Inria</li>
<li>arxiv: <a href="https://arxiv.org/abs/1708.06977" target="_blank" rel="external">https://arxiv.org/abs/1708.06977</a></li>
</ul>
<p><strong>Zoom Out-and-In Network with Map Attention Decision for Region Proposal and Object Detection</strong></p>
<p><a href="https://arxiv.org/abs/1709.04347" target="_blank" rel="external">https://arxiv.org/abs/1709.04347</a></p>
<p><strong>StairNet: Top-Down Semantic Aggregation for Accurate One Shot Detection</strong></p>
<p><a href="https://arxiv.org/abs/1709.05788" target="_blank" rel="external">https://arxiv.org/abs/1709.05788</a></p>
<p><strong>Dynamic Zoom-in Network for Fast Object Detection in Large Images</strong></p>
<p><a href="https://arxiv.org/abs/1711.05187" target="_blank" rel="external">https://arxiv.org/abs/1711.05187</a></p>
<p><strong>Zero-Annotation Object Detection with Web Knowledge Transfer</strong></p>
<ul>
<li>intro: NTU, Singapore &amp; Amazon</li>
<li>keywords: multi-instance multi-label domain adaption learning framework</li>
<li>arxiv: <a href="https://arxiv.org/abs/1711.05954" target="_blank" rel="external">https://arxiv.org/abs/1711.05954</a></li>
</ul>
<h2 id="MegDet"><a href="#MegDet" class="headerlink" title="MegDet"></a>MegDet</h2><p><strong>MegDet: A Large Mini-Batch Object Detector</strong></p>
<ul>
<li>intro: Peking University &amp; Tsinghua University &amp; Megvii Inc</li>
<li>arxiv: <a href="https://arxiv.org/abs/1711.07240" target="_blank" rel="external">https://arxiv.org/abs/1711.07240</a></li>
</ul>
<p><strong>Single-Shot Refinement Neural Network for Object Detection</strong></p>
<ul>
<li>arxiv: <a href="https://arxiv.org/abs/1711.06897" target="_blank" rel="external">https://arxiv.org/abs/1711.06897</a></li>
<li>github: <a href="https://github.com/sfzhang15/RefineDet" target="_blank" rel="external">https://github.com/sfzhang15/RefineDet</a></li>
</ul>
<p><strong>Receptive Field Block Net for Accurate and Fast Object Detection</strong></p>
<ul>
<li>intro: RFBNet</li>
<li>arxiv: <a href="https://arxiv.org/abs/1711.07767" target="_blank" rel="external">https://arxiv.org/abs/1711.07767</a></li>
<li>github: <a href="https://github.com//ruinmessi/RFBNet" target="_blank" rel="external">https://github.com//ruinmessi/RFBNet</a></li>
</ul>
<p><strong>An Analysis of Scale Invariance in Object Detection - SNIP</strong></p>
<ul>
<li>arxiv: <a href="https://arxiv.org/abs/1711.08189" target="_blank" rel="external">https://arxiv.org/abs/1711.08189</a></li>
<li>github: <a href="https://github.com/bharatsingh430/snip" target="_blank" rel="external">https://github.com/bharatsingh430/snip</a></li>
</ul>
<p><strong>Feature Selective Networks for Object Detection</strong></p>
<p><a href="https://arxiv.org/abs/1711.08879" target="_blank" rel="external">https://arxiv.org/abs/1711.08879</a></p>
<p><strong>Learning a Rotation Invariant Detector with Rotatable Bounding Box</strong></p>
<ul>
<li>arxiv: <a href="https://arxiv.org/abs/1711.09405" target="_blank" rel="external">https://arxiv.org/abs/1711.09405</a></li>
<li>github: <a href="https://github.com/liulei01/DRBox" target="_blank" rel="external">https://github.com/liulei01/DRBox</a></li>
</ul>
<p><strong>Scalable Object Detection for Stylized Objects</strong></p>
<ul>
<li>intro: Microsoft AI &amp; Research Munich</li>
<li>arxiv: <a href="https://arxiv.org/abs/1711.09822" target="_blank" rel="external">https://arxiv.org/abs/1711.09822</a></li>
</ul>
<p><strong>Learning Object Detectors from Scratch with Gated Recurrent Feature Pyramids</strong></p>
<ul>
<li>arxiv: <a href="https://arxiv.org/abs/1712.00886" target="_blank" rel="external">https://arxiv.org/abs/1712.00886</a></li>
<li>github: <a href="https://github.com/szq0214/GRP-DSOD" target="_blank" rel="external">https://github.com/szq0214/GRP-DSOD</a></li>
</ul>
<p><strong>Deep Regionlets for Object Detection</strong></p>
<ul>
<li>keywords: region selection network, gating network</li>
<li>arxiv: <a href="https://arxiv.org/abs/1712.02408" target="_blank" rel="external">https://arxiv.org/abs/1712.02408</a></li>
</ul>
<p><strong>Training and Testing Object Detectors with Virtual Images</strong></p>
<ul>
<li>intro: IEEE/CAA Journal of Automatica Sinica</li>
<li>arxiv: <a href="https://arxiv.org/abs/1712.08470" target="_blank" rel="external">https://arxiv.org/abs/1712.08470</a></li>
</ul>
<p><strong>Large-Scale Object Discovery and Detector Adaptation from Unlabeled Video</strong></p>
<ul>
<li>keywords: object mining, object tracking, unsupervised object discovery by appearance-based clustering, self-supervised detector adaptation</li>
<li>arxiv: <a href="https://arxiv.org/abs/1712.08832" target="_blank" rel="external">https://arxiv.org/abs/1712.08832</a></li>
</ul>
<p><strong>Spot the Difference by Object Detection</strong></p>
<ul>
<li>intro: Tsinghua University &amp; JD Group</li>
<li>arxiv: <a href="https://arxiv.org/abs/1801.01051" target="_blank" rel="external">https://arxiv.org/abs/1801.01051</a></li>
</ul>
<p><strong>Localization-Aware Active Learning for Object Detection</strong></p>
<ul>
<li>arxiv: <a href="https://arxiv.org/abs/1801.05124" target="_blank" rel="external">https://arxiv.org/abs/1801.05124</a></li>
</ul>
<p><strong>Object Detection with Mask-based Feature Encoding</strong></p>
<p><a href="https://arxiv.org/abs/1802.03934" target="_blank" rel="external">https://arxiv.org/abs/1802.03934</a></p>
<p><strong>LSTD: A Low-Shot Transfer Detector for Object Detection</strong></p>
<ul>
<li>intro: AAAI 2018</li>
<li>arxiv: <a href="https://arxiv.org/abs/1803.01529" target="_blank" rel="external">https://arxiv.org/abs/1803.01529</a></li>
</ul>
<p><strong>Domain Adaptive Faster R-CNN for Object Detection in the Wild</strong></p>
<ul>
<li>intro: CVPR 2018. ETH Zurich &amp; ESAT/PSI</li>
<li>arxiv: <a href="https://arxiv.org/abs/1803.03243" target="_blank" rel="external">https://arxiv.org/abs/1803.03243</a></li>
</ul>
<p><strong>Pseudo Mask Augmented Object Detection</strong></p>
<p><a href="https://arxiv.org/abs/1803.05858" target="_blank" rel="external">https://arxiv.org/abs/1803.05858</a></p>
<p><strong>Revisiting RCNN: On Awakening the Classification Power of Faster RCNN</strong></p>
<p><a href="https://arxiv.org/abs/1803.06799" target="_blank" rel="external">https://arxiv.org/abs/1803.06799</a></p>
<p><strong>Zero-Shot Detection</strong></p>
<ul>
<li>intro: Australian National University</li>
<li>keywords: YOLO</li>
<li>arxiv: <a href="https://arxiv.org/abs/1803.07113" target="_blank" rel="external">https://arxiv.org/abs/1803.07113</a></li>
</ul>
<p><strong>Learning Region Features for Object Detection</strong></p>
<ul>
<li>intro: Peking University &amp; MSRA</li>
<li>arxiv: <a href="https://arxiv.org/abs/1803.07066" target="_blank" rel="external">https://arxiv.org/abs/1803.07066</a></li>
</ul>
<p><strong>Single-Shot Bidirectional Pyramid Networks for High-Quality Object Detection</strong></p>
<p><a href="https://arxiv.org/abs/1803.08208" target="_blank" rel="external">https://arxiv.org/abs/1803.08208</a></p>
<p><strong>Object Detection for Comics using Manga109 Annotations</strong></p>
<ul>
<li>intro: University of Tokyo &amp; National Institute of Informatics, Japan</li>
<li>arxiv: <a href="https://arxiv.org/abs/1803.08670" target="_blank" rel="external">https://arxiv.org/abs/1803.08670</a></li>
</ul>
<h1 id="Non-Maximum-Suppression-NMS"><a href="#Non-Maximum-Suppression-NMS" class="headerlink" title="Non-Maximum Suppression (NMS)"></a>Non-Maximum Suppression (NMS)</h1><p><strong>End-to-End Integration of a Convolutional Network, Deformable Parts Model and Non-Maximum Suppression</strong></p>
<ul>
<li>intro: CVPR 2015</li>
<li>arxiv: <a href="http://arxiv.org/abs/1411.5309" target="_blank" rel="external">http://arxiv.org/abs/1411.5309</a></li>
<li>paper: <a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Wan_End-to-End_Integration_of_2015_CVPR_paper.pdf" target="_blank" rel="external">http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Wan_End-to-End_Integration_of_2015_CVPR_paper.pdf</a></li>
</ul>
<p><strong>A convnet for non-maximum suppression</strong></p>
<ul>
<li>arxiv: <a href="http://arxiv.org/abs/1511.06437" target="_blank" rel="external">http://arxiv.org/abs/1511.06437</a></li>
</ul>
<p><strong>Improving Object Detection With One Line of Code</strong></p>
<p><strong>Soft-NMS – Improving Object Detection With One Line of Code</strong></p>
<ul>
<li>intro: ICCV 2017. University of Maryland</li>
<li>keywords: Soft-NMS</li>
<li>arxiv: <a href="https://arxiv.org/abs/1704.04503" target="_blank" rel="external">https://arxiv.org/abs/1704.04503</a></li>
<li>github: <a href="https://github.com/bharatsingh430/soft-nms" target="_blank" rel="external">https://github.com/bharatsingh430/soft-nms</a></li>
</ul>
<p><strong>Learning non-maximum suppression</strong></p>
<ul>
<li>intro: CVPR 2017</li>
<li>project page: <a href="https://www.mpi-inf.mpg.de/departments/computer-vision-and-multimodal-computing/research/object-recognition-and-scene-understanding/learning-nms/" target="_blank" rel="external">https://www.mpi-inf.mpg.de/departments/computer-vision-and-multimodal-computing/research/object-recognition-and-scene-understanding/learning-nms/</a></li>
<li>arxiv: <a href="https://arxiv.org/abs/1705.02950" target="_blank" rel="external">https://arxiv.org/abs/1705.02950</a></li>
<li>github: <a href="https://github.com/hosang/gossipnet" target="_blank" rel="external">https://github.com/hosang/gossipnet</a></li>
</ul>
<p><strong>Relation Networks for Object Detection</strong></p>
<p><a href="https://arxiv.org/abs/1711.11575" target="_blank" rel="external">https://arxiv.org/abs/1711.11575</a></p>
<h1 id="Adversarial-Examples"><a href="#Adversarial-Examples" class="headerlink" title="Adversarial Examples"></a>Adversarial Examples</h1><p><strong>Adversarial Examples that Fool Detectors</strong></p>
<ul>
<li>intro: University of Illinois</li>
<li>arxiv: <a href="https://arxiv.org/abs/1712.02494" target="_blank" rel="external">https://arxiv.org/abs/1712.02494</a></li>
</ul>
<p><strong>Adversarial Examples Are Not Easily Detected: Bypassing Ten Detection Methods</strong></p>
<ul>
<li>project page: <a href="http://nicholas.carlini.com/code/nn_breaking_detection/" target="_blank" rel="external">http://nicholas.carlini.com/code/nn_breaking_detection/</a></li>
<li>arxiv: <a href="https://arxiv.org/abs/1705.07263" target="_blank" rel="external">https://arxiv.org/abs/1705.07263</a></li>
<li>github: <a href="https://github.com/carlini/nn_breaking_detection" target="_blank" rel="external">https://github.com/carlini/nn_breaking_detection</a></li>
</ul>
<h1 id="Weakly-Supervised-Object-Detection"><a href="#Weakly-Supervised-Object-Detection" class="headerlink" title="Weakly Supervised Object Detection"></a>Weakly Supervised Object Detection</h1><p><strong>Track and Transfer: Watching Videos to Simulate Strong Human Supervision for Weakly-Supervised Object Detection</strong></p>
<ul>
<li>intro: CVPR 2016</li>
<li>arxiv: <a href="http://arxiv.org/abs/1604.05766" target="_blank" rel="external">http://arxiv.org/abs/1604.05766</a></li>
</ul>
<p><strong>Weakly supervised object detection using pseudo-strong labels</strong></p>
<ul>
<li>arxiv: <a href="http://arxiv.org/abs/1607.04731" target="_blank" rel="external">http://arxiv.org/abs/1607.04731</a></li>
</ul>
<p><strong>Saliency Guided End-to-End Learning for Weakly Supervised Object Detection</strong></p>
<ul>
<li>intro: IJCAI 2017</li>
<li>arxiv: <a href="https://arxiv.org/abs/1706.06768" target="_blank" rel="external">https://arxiv.org/abs/1706.06768</a></li>
</ul>
<p><strong>Visual and Semantic Knowledge Transfer for Large Scale Semi-supervised Object Detection</strong></p>
<ul>
<li>intro: TPAMI 2017. National Institutes of Health (NIH) Clinical Center</li>
<li>arxiv: <a href="https://arxiv.org/abs/1801.03145" target="_blank" rel="external">https://arxiv.org/abs/1801.03145</a></li>
</ul>
<h1 id="Video-Object-Detection"><a href="#Video-Object-Detection" class="headerlink" title="Video Object Detection"></a>Video Object Detection</h1><p><strong>Learning Object Class Detectors from Weakly Annotated Video</strong></p>
<ul>
<li>intro: CVPR 2012</li>
<li>paper: <a href="https://www.vision.ee.ethz.ch/publications/papers/proceedings/eth_biwi_00905.pdf" target="_blank" rel="external">https://www.vision.ee.ethz.ch/publications/papers/proceedings/eth_biwi_00905.pdf</a></li>
</ul>
<p><strong>Analysing domain shift factors between videos and images for object detection</strong></p>
<ul>
<li>arxiv: <a href="https://arxiv.org/abs/1501.01186" target="_blank" rel="external">https://arxiv.org/abs/1501.01186</a></li>
</ul>
<p><strong>Video Object Recognition</strong></p>
<ul>
<li>slides: <a href="http://vision.princeton.edu/courses/COS598/2015sp/slides/VideoRecog/Video%20Object%20Recognition.pptx" target="_blank" rel="external">http://vision.princeton.edu/courses/COS598/2015sp/slides/VideoRecog/Video%20Object%20Recognition.pptx</a></li>
</ul>
<p><strong>Deep Learning for Saliency Prediction in Natural Video</strong></p>
<ul>
<li>intro: Submitted on 12 Jan 2016</li>
<li>keywords: Deep learning, saliency map, optical flow, convolution network, contrast features</li>
<li>paper: <a href="https://hal.archives-ouvertes.fr/hal-01251614/document" target="_blank" rel="external">https://hal.archives-ouvertes.fr/hal-01251614/document</a></li>
</ul>
<p><strong>T-CNN: Tubelets with Convolutional Neural Networks for Object Detection from Videos</strong></p>
<ul>
<li>intro: Winning solution in ILSVRC2015 Object Detection from Video(VID) Task</li>
<li>arxiv: <a href="http://arxiv.org/abs/1604.02532" target="_blank" rel="external">http://arxiv.org/abs/1604.02532</a></li>
<li>github: <a href="https://github.com/myfavouritekk/T-CNN" target="_blank" rel="external">https://github.com/myfavouritekk/T-CNN</a></li>
</ul>
<p><strong>Object Detection from Video Tubelets with Convolutional Neural Networks</strong></p>
<ul>
<li>intro: CVPR 2016 Spotlight paper</li>
<li>arxiv: <a href="https://arxiv.org/abs/1604.04053" target="_blank" rel="external">https://arxiv.org/abs/1604.04053</a></li>
<li>paper: <a href="http://www.ee.cuhk.edu.hk/~wlouyang/Papers/KangVideoDet_CVPR16.pdf" target="_blank" rel="external">http://www.ee.cuhk.edu.hk/~wlouyang/Papers/KangVideoDet_CVPR16.pdf</a></li>
<li>gihtub: <a href="https://github.com/myfavouritekk/vdetlib" target="_blank" rel="external">https://github.com/myfavouritekk/vdetlib</a></li>
</ul>
<p><strong>Object Detection in Videos with Tubelets and Multi-context Cues</strong></p>
<ul>
<li>intro: SenseTime Group</li>
<li>slides: <a href="http://www.ee.cuhk.edu.hk/~xgwang/CUvideo.pdf" target="_blank" rel="external">http://www.ee.cuhk.edu.hk/~xgwang/CUvideo.pdf</a></li>
<li>slides: <a href="http://image-net.org/challenges/talks/Object%20Detection%20in%20Videos%20with%20Tubelets%20and%20Multi-context%20Cues%20-%20Final.pdf" target="_blank" rel="external">http://image-net.org/challenges/talks/Object%20Detection%20in%20Videos%20with%20Tubelets%20and%20Multi-context%20Cues%20-%20Final.pdf</a></li>
</ul>
<p><strong>Context Matters: Refining Object Detection in Video with Recurrent Neural Networks</strong></p>
<ul>
<li>intro: BMVC 2016</li>
<li>keywords: pseudo-labeler</li>
<li>arxiv: <a href="http://arxiv.org/abs/1607.04648" target="_blank" rel="external">http://arxiv.org/abs/1607.04648</a></li>
<li>paper: <a href="http://vision.cornell.edu/se3/wp-content/uploads/2016/07/video_object_detection_BMVC.pdf" target="_blank" rel="external">http://vision.cornell.edu/se3/wp-content/uploads/2016/07/video_object_detection_BMVC.pdf</a></li>
</ul>
<p><strong>CNN Based Object Detection in Large Video Images</strong></p>
<ul>
<li>intro: WangTao @ 爱奇艺</li>
<li>keywords: object retrieval, object detection, scene classification</li>
<li>slides: <a href="http://on-demand.gputechconf.com/gtc/2016/presentation/s6362-wang-tao-cnn-based-object-detection-large-video-images.pdf" target="_blank" rel="external">http://on-demand.gputechconf.com/gtc/2016/presentation/s6362-wang-tao-cnn-based-object-detection-large-video-images.pdf</a></li>
</ul>
<p><strong>Object Detection in Videos with Tubelet Proposal Networks</strong></p>
<ul>
<li>arxiv: <a href="https://arxiv.org/abs/1702.06355" target="_blank" rel="external">https://arxiv.org/abs/1702.06355</a></li>
</ul>
<p><strong>Flow-Guided Feature Aggregation for Video Object Detection</strong></p>
<ul>
<li>intro: MSRA</li>
<li>arxiv: <a href="https://arxiv.org/abs/1703.10025" target="_blank" rel="external">https://arxiv.org/abs/1703.10025</a></li>
</ul>
<p><strong>Video Object Detection using Faster R-CNN</strong></p>
<ul>
<li>blog: <a href="http://andrewliao11.github.io/object_detection/faster_rcnn/" target="_blank" rel="external">http://andrewliao11.github.io/object_detection/faster_rcnn/</a></li>
<li>github: <a href="https://github.com/andrewliao11/py-faster-rcnn-imagenet" target="_blank" rel="external">https://github.com/andrewliao11/py-faster-rcnn-imagenet</a></li>
</ul>
<p><strong>Improving Context Modeling for Video Object Detection and Tracking</strong></p>
<p><a href="http://image-net.org/challenges/talks_2017/ilsvrc2017_short(poster" target="_blank" rel="external">http://image-net.org/challenges/talks_2017/ilsvrc2017_short(poster).pdf</a>.pdf)</p>
<p><strong>Temporal Dynamic Graph LSTM for Action-driven Video Object Detection</strong></p>
<ul>
<li>intro: ICCV 2017</li>
<li>arxiv: <a href="https://arxiv.org/abs/1708.00666" target="_blank" rel="external">https://arxiv.org/abs/1708.00666</a></li>
</ul>
<p><strong>Mobile Video Object Detection with Temporally-Aware Feature Maps</strong></p>
<p><a href="https://arxiv.org/abs/1711.06368" target="_blank" rel="external">https://arxiv.org/abs/1711.06368</a></p>
<p><strong>Towards High Performance Video Object Detection</strong></p>
<p><a href="https://arxiv.org/abs/1711.11577" target="_blank" rel="external">https://arxiv.org/abs/1711.11577</a></p>
<p><strong>Impression Network for Video Object Detection</strong></p>
<p><a href="https://arxiv.org/abs/1712.05896" target="_blank" rel="external">https://arxiv.org/abs/1712.05896</a></p>
<p><strong>Spatial-Temporal Memory Networks for Video Object Detection</strong></p>
<p><a href="https://arxiv.org/abs/1712.06317" target="_blank" rel="external">https://arxiv.org/abs/1712.06317</a></p>
<p><strong>3D-DETNet: a Single Stage Video-Based Vehicle Detector</strong></p>
<p><a href="https://arxiv.org/abs/1801.01769" target="_blank" rel="external">https://arxiv.org/abs/1801.01769</a></p>
<p><strong>Object Detection in Videos by Short and Long Range Object Linking</strong></p>
<p><a href="https://arxiv.org/abs/1801.09823" target="_blank" rel="external">https://arxiv.org/abs/1801.09823</a></p>
<p><strong>Object Detection in Video with Spatiotemporal Sampling Networks</strong></p>
<ul>
<li>intro: University of Pennsylvania, 2Dartmouth College</li>
<li>arxiv: <a href="https://arxiv.org/abs/1803.05549" target="_blank" rel="external">https://arxiv.org/abs/1803.05549</a></li>
</ul>
<h1 id="Object-Detection-in-3D"><a href="#Object-Detection-in-3D" class="headerlink" title="Object Detection in 3D"></a>Object Detection in 3D</h1><p><strong>Vote3Deep: Fast Object Detection in 3D Point Clouds Using Efficient Convolutional Neural Networks</strong></p>
<ul>
<li>arxiv: <a href="https://arxiv.org/abs/1609.06666" target="_blank" rel="external">https://arxiv.org/abs/1609.06666</a></li>
</ul>
<p><strong>Complex-YOLO: Real-time 3D Object Detection on Point Clouds</strong></p>
<ul>
<li>intro: Valeo Schalter und Sensoren GmbH &amp; Ilmenau University of Technology</li>
<li>arxiv: <a href="https://arxiv.org/abs/1803.06199" target="_blank" rel="external">https://arxiv.org/abs/1803.06199</a></li>
</ul>
<h1 id="Object-Detection-on-RGB-D"><a href="#Object-Detection-on-RGB-D" class="headerlink" title="Object Detection on RGB-D"></a>Object Detection on RGB-D</h1><p><strong>Learning Rich Features from RGB-D Images for Object Detection and Segmentation</strong></p>
<ul>
<li>arxiv: <a href="http://arxiv.org/abs/1407.5736" target="_blank" rel="external">http://arxiv.org/abs/1407.5736</a></li>
</ul>
<p><strong>Differential Geometry Boosts Convolutional Neural Networks for Object Detection</strong></p>
<ul>
<li>intro: CVPR 2016</li>
<li>paper: <a href="http://www.cv-foundation.org/openaccess/content_cvpr_2016_workshops/w23/html/Wang_Differential_Geometry_Boosts_CVPR_2016_paper.html" target="_blank" rel="external">http://www.cv-foundation.org/openaccess/content_cvpr_2016_workshops/w23/html/Wang_Differential_Geometry_Boosts_CVPR_2016_paper.html</a></li>
</ul>
<p><strong>A Self-supervised Learning System for Object Detection using Physics Simulation and Multi-view Pose Estimation</strong></p>
<p><a href="https://arxiv.org/abs/1703.03347" target="_blank" rel="external">https://arxiv.org/abs/1703.03347</a></p>
<h1 id="Salient-Object-Detection"><a href="#Salient-Object-Detection" class="headerlink" title="Salient Object Detection"></a>Salient Object Detection</h1><p>This task involves predicting the salient regions of an image given by human eye fixations.</p>
<p><strong>Best Deep Saliency Detection Models (CVPR 2016 &amp; 2015)</strong></p>
<p><a href="http://i.cs.hku.hk/~yzyu/vision.html" target="_blank" rel="external">http://i.cs.hku.hk/~yzyu/vision.html</a></p>
<p><strong>Large-scale optimization of hierarchical features for saliency prediction in natural images</strong></p>
<ul>
<li>paper: <a href="http://coxlab.org/pdfs/cvpr2014_vig_saliency.pdf" target="_blank" rel="external">http://coxlab.org/pdfs/cvpr2014_vig_saliency.pdf</a></li>
</ul>
<p><strong>Predicting Eye Fixations using Convolutional Neural Networks</strong></p>
<ul>
<li>paper: <a href="http://www.escience.cn/system/file?fileId=72648" target="_blank" rel="external">http://www.escience.cn/system/file?fileId=72648</a></li>
</ul>
<p><strong>Saliency Detection by Multi-Context Deep Learning</strong></p>
<ul>
<li>paper: <a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Zhao_Saliency_Detection_by_2015_CVPR_paper.pdf" target="_blank" rel="external">http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Zhao_Saliency_Detection_by_2015_CVPR_paper.pdf</a></li>
</ul>
<p><strong>DeepSaliency: Multi-Task Deep Neural Network Model for Salient Object Detection</strong></p>
<ul>
<li>arxiv: <a href="http://arxiv.org/abs/1510.05484" target="_blank" rel="external">http://arxiv.org/abs/1510.05484</a></li>
</ul>
<p><strong>SuperCNN: A Superpixelwise Convolutional Neural Network for Salient Object Detection</strong></p>
<ul>
<li>paper: <a href="www.shengfenghe.com/supercnn-a-superpixelwise-convolutional-neural-network-for-salient-object-detection.html">www.shengfenghe.com/supercnn-a-superpixelwise-convolutional-neural-network-for-salient-object-detection.html</a></li>
</ul>
<p><strong>Shallow and Deep Convolutional Networks for Saliency Prediction</strong></p>
<ul>
<li>intro: CVPR 2016</li>
<li>arxiv: <a href="http://arxiv.org/abs/1603.00845" target="_blank" rel="external">http://arxiv.org/abs/1603.00845</a></li>
<li>github: <a href="https://github.com/imatge-upc/saliency-2016-cvpr" target="_blank" rel="external">https://github.com/imatge-upc/saliency-2016-cvpr</a></li>
</ul>
<p><strong>Recurrent Attentional Networks for Saliency Detection</strong></p>
<ul>
<li>intro: CVPR 2016. recurrent attentional convolutional-deconvolution network (RACDNN)</li>
<li>arxiv: <a href="http://arxiv.org/abs/1604.03227" target="_blank" rel="external">http://arxiv.org/abs/1604.03227</a></li>
</ul>
<p><strong>Two-Stream Convolutional Networks for Dynamic Saliency Prediction</strong></p>
<ul>
<li>arxiv: <a href="http://arxiv.org/abs/1607.04730" target="_blank" rel="external">http://arxiv.org/abs/1607.04730</a></li>
</ul>
<p><strong>Unconstrained Salient Object Detection</strong></p>
<p><strong>Unconstrained Salient Object Detection via Proposal Subset Optimization</strong></p>
<p><img src="http://cs-people.bu.edu/jmzhang/images/pasted%20image%201465x373.jpg" alt=""></p>
<ul>
<li>intro: CVPR 2016</li>
<li>project page: <a href="http://cs-people.bu.edu/jmzhang/sod.html" target="_blank" rel="external">http://cs-people.bu.edu/jmzhang/sod.html</a></li>
<li>paper: <a href="http://cs-people.bu.edu/jmzhang/SOD/CVPR16SOD_camera_ready.pdf" target="_blank" rel="external">http://cs-people.bu.edu/jmzhang/SOD/CVPR16SOD_camera_ready.pdf</a></li>
<li>github: <a href="https://github.com/jimmie33/SOD" target="_blank" rel="external">https://github.com/jimmie33/SOD</a></li>
<li>caffe model zoo: <a href="https://github.com/BVLC/caffe/wiki/Model-Zoo#cnn-object-proposal-models-for-salient-object-detection" target="_blank" rel="external">https://github.com/BVLC/caffe/wiki/Model-Zoo#cnn-object-proposal-models-for-salient-object-detection</a></li>
</ul>
<p><strong>DHSNet: Deep Hierarchical Saliency Network for Salient Object Detection</strong></p>
<ul>
<li>paper: <a href="http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Liu_DHSNet_Deep_Hierarchical_CVPR_2016_paper.pdf" target="_blank" rel="external">http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Liu_DHSNet_Deep_Hierarchical_CVPR_2016_paper.pdf</a></li>
</ul>
<p><strong>Salient Object Subitizing</strong></p>
<p><img src="http://cs-people.bu.edu/jmzhang/images/frontpage.png?crc=123070793" alt=""></p>
<ul>
<li>intro: CVPR 2015</li>
<li>intro: predicting the existence and the number of salient objects in an image using holistic cues</li>
<li>project page: <a href="http://cs-people.bu.edu/jmzhang/sos.html" target="_blank" rel="external">http://cs-people.bu.edu/jmzhang/sos.html</a></li>
<li>arxiv: <a href="http://arxiv.org/abs/1607.07525" target="_blank" rel="external">http://arxiv.org/abs/1607.07525</a></li>
<li>paper: <a href="http://cs-people.bu.edu/jmzhang/SOS/SOS_preprint.pdf" target="_blank" rel="external">http://cs-people.bu.edu/jmzhang/SOS/SOS_preprint.pdf</a></li>
<li>caffe model zoo: <a href="https://github.com/BVLC/caffe/wiki/Model-Zoo#cnn-models-for-salient-object-subitizing" target="_blank" rel="external">https://github.com/BVLC/caffe/wiki/Model-Zoo#cnn-models-for-salient-object-subitizing</a></li>
</ul>
<p><strong>Deeply-Supervised Recurrent Convolutional Neural Network for Saliency Detection</strong></p>
<ul>
<li>intro: ACMMM 2016. deeply-supervised recurrent convolutional neural network (DSRCNN)</li>
<li>arxiv: <a href="http://arxiv.org/abs/1608.05177" target="_blank" rel="external">http://arxiv.org/abs/1608.05177</a></li>
</ul>
<p><strong>Saliency Detection via Combining Region-Level and Pixel-Level Predictions with CNNs</strong></p>
<ul>
<li>intro: ECCV 2016</li>
<li>arxiv: <a href="http://arxiv.org/abs/1608.05186" target="_blank" rel="external">http://arxiv.org/abs/1608.05186</a></li>
</ul>
<p><strong>Edge Preserving and Multi-Scale Contextual Neural Network for Salient Object Detection</strong></p>
<ul>
<li>arxiv: <a href="http://arxiv.org/abs/1608.08029" target="_blank" rel="external">http://arxiv.org/abs/1608.08029</a></li>
</ul>
<p><strong>A Deep Multi-Level Network for Saliency Prediction</strong></p>
<ul>
<li>arxiv: <a href="http://arxiv.org/abs/1609.01064" target="_blank" rel="external">http://arxiv.org/abs/1609.01064</a></li>
</ul>
<p><strong>Visual Saliency Detection Based on Multiscale Deep CNN Features</strong></p>
<ul>
<li>intro: IEEE Transactions on Image Processing</li>
<li>arxiv: <a href="http://arxiv.org/abs/1609.02077" target="_blank" rel="external">http://arxiv.org/abs/1609.02077</a></li>
</ul>
<p><strong>A Deep Spatial Contextual Long-term Recurrent Convolutional Network for Saliency Detection</strong></p>
<ul>
<li>intro: DSCLRCN</li>
<li>arxiv: <a href="https://arxiv.org/abs/1610.01708" target="_blank" rel="external">https://arxiv.org/abs/1610.01708</a></li>
</ul>
<p><strong>Deeply supervised salient object detection with short connections</strong></p>
<ul>
<li>arxiv: <a href="https://arxiv.org/abs/1611.04849" target="_blank" rel="external">https://arxiv.org/abs/1611.04849</a></li>
</ul>
<p><strong>Weakly Supervised Top-down Salient Object Detection</strong></p>
<ul>
<li>intro: Nanyang Technological University</li>
<li>arxiv: <a href="https://arxiv.org/abs/1611.05345" target="_blank" rel="external">https://arxiv.org/abs/1611.05345</a></li>
</ul>
<p><strong>SalGAN: Visual Saliency Prediction with Generative Adversarial Networks</strong></p>
<ul>
<li>project page: <a href="https://imatge-upc.github.io/saliency-salgan-2017/" target="_blank" rel="external">https://imatge-upc.github.io/saliency-salgan-2017/</a></li>
<li>arxiv: <a href="https://arxiv.org/abs/1701.01081" target="_blank" rel="external">https://arxiv.org/abs/1701.01081</a></li>
</ul>
<p><strong>Visual Saliency Prediction Using a Mixture of Deep Neural Networks</strong></p>
<ul>
<li>arxiv: <a href="https://arxiv.org/abs/1702.00372" target="_blank" rel="external">https://arxiv.org/abs/1702.00372</a></li>
</ul>
<p><strong>A Fast and Compact Salient Score Regression Network Based on Fully Convolutional Network</strong></p>
<ul>
<li>arxiv: <a href="https://arxiv.org/abs/1702.00615" target="_blank" rel="external">https://arxiv.org/abs/1702.00615</a></li>
</ul>
<p><strong>Saliency Detection by Forward and Backward Cues in Deep-CNNs</strong></p>
<p><a href="https://arxiv.org/abs/1703.00152" target="_blank" rel="external">https://arxiv.org/abs/1703.00152</a></p>
<p><strong>Supervised Adversarial Networks for Image Saliency Detection</strong></p>
<p><a href="https://arxiv.org/abs/1704.07242" target="_blank" rel="external">https://arxiv.org/abs/1704.07242</a></p>
<p><strong>Group-wise Deep Co-saliency Detection</strong></p>
<p><a href="https://arxiv.org/abs/1707.07381" target="_blank" rel="external">https://arxiv.org/abs/1707.07381</a></p>
<p><strong>Towards the Success Rate of One: Real-time Unconstrained Salient Object Detection</strong></p>
<ul>
<li>intro: University of Maryland College Park &amp; eBay Inc</li>
<li>arxiv: <a href="https://arxiv.org/abs/1708.00079" target="_blank" rel="external">https://arxiv.org/abs/1708.00079</a></li>
</ul>
<p><strong>Amulet: Aggregating Multi-level Convolutional Features for Salient Object Detection</strong></p>
<ul>
<li>intro: ICCV 2017</li>
<li>arixv: <a href="https://arxiv.org/abs/1708.02001" target="_blank" rel="external">https://arxiv.org/abs/1708.02001</a></li>
</ul>
<p><strong>Learning Uncertain Convolutional Features for Accurate Saliency Detection</strong></p>
<ul>
<li>intro: Accepted as a poster in ICCV 2017</li>
<li>arxiv: <a href="https://arxiv.org/abs/1708.02031" target="_blank" rel="external">https://arxiv.org/abs/1708.02031</a></li>
</ul>
<p><strong>Deep Edge-Aware Saliency Detection</strong></p>
<p><a href="https://arxiv.org/abs/1708.04366" target="_blank" rel="external">https://arxiv.org/abs/1708.04366</a></p>
<p><strong>Self-explanatory Deep Salient Object Detection</strong></p>
<ul>
<li>intro: National University of Defense Technology, China &amp; National University of Singapore</li>
<li>arxiv: <a href="https://arxiv.org/abs/1708.05595" target="_blank" rel="external">https://arxiv.org/abs/1708.05595</a></li>
</ul>
<p><strong>PiCANet: Learning Pixel-wise Contextual Attention in ConvNets and Its Application in Saliency Detection</strong></p>
<p><a href="https://arxiv.org/abs/1708.06433" target="_blank" rel="external">https://arxiv.org/abs/1708.06433</a></p>
<p><strong>DeepFeat: A Bottom Up and Top Down Saliency Model Based on Deep Features of Convolutional Neural Nets</strong></p>
<p><a href="https://arxiv.org/abs/1709.02495" target="_blank" rel="external">https://arxiv.org/abs/1709.02495</a></p>
<p><strong>Deep saliency: What is learnt by a deep network about saliency?</strong></p>
<ul>
<li>intro: 2nd Workshop on Visualisation for Deep Learning in the 34th International Conference On Machine Learning</li>
<li>arxiv: <a href="https://arxiv.org/abs/1801.04261" target="_blank" rel="external">https://arxiv.org/abs/1801.04261</a></li>
</ul>
<h1 id="Video-Saliency-Detection"><a href="#Video-Saliency-Detection" class="headerlink" title="Video Saliency Detection"></a>Video Saliency Detection</h1><p><strong>Deep Learning For Video Saliency Detection</strong></p>
<ul>
<li>arxiv: <a href="https://arxiv.org/abs/1702.00871" target="_blank" rel="external">https://arxiv.org/abs/1702.00871</a></li>
</ul>
<p><strong>Video Salient Object Detection Using Spatiotemporal Deep Features</strong></p>
<p><a href="https://arxiv.org/abs/1708.01447" target="_blank" rel="external">https://arxiv.org/abs/1708.01447</a></p>
<p><strong>Predicting Video Saliency with Object-to-Motion CNN and Two-layer Convolutional LSTM</strong></p>
<p><a href="https://arxiv.org/abs/1709.06316" target="_blank" rel="external">https://arxiv.org/abs/1709.06316</a></p>
<h1 id="Visual-Relationship-Detection"><a href="#Visual-Relationship-Detection" class="headerlink" title="Visual Relationship Detection"></a>Visual Relationship Detection</h1><p><strong>Visual Relationship Detection with Language Priors</strong></p>
<ul>
<li>intro: ECCV 2016 oral</li>
<li>paper: <a href="https://cs.stanford.edu/people/ranjaykrishna/vrd/vrd.pdf" target="_blank" rel="external">https://cs.stanford.edu/people/ranjaykrishna/vrd/vrd.pdf</a></li>
<li>github: <a href="https://github.com/Prof-Lu-Cewu/Visual-Relationship-Detection" target="_blank" rel="external">https://github.com/Prof-Lu-Cewu/Visual-Relationship-Detection</a></li>
</ul>
<p><strong>ViP-CNN: A Visual Phrase Reasoning Convolutional Neural Network for Visual Relationship Detection</strong></p>
<ul>
<li>intro: Visual Phrase reasoning Convolutional Neural Network (ViP-CNN), Visual Phrase Reasoning Structure (VPRS)</li>
<li>arxiv: <a href="https://arxiv.org/abs/1702.07191" target="_blank" rel="external">https://arxiv.org/abs/1702.07191</a></li>
</ul>
<p><strong>Visual Translation Embedding Network for Visual Relation Detection</strong></p>
<ul>
<li>arxiv: <a href="https://www.arxiv.org/abs/1702.08319" target="_blank" rel="external">https://www.arxiv.org/abs/1702.08319</a></li>
</ul>
<p><strong>Deep Variation-structured Reinforcement Learning for Visual Relationship and Attribute Detection</strong></p>
<ul>
<li>intro: CVPR 2017 spotlight paper</li>
<li>arxiv: <a href="https://arxiv.org/abs/1703.03054" target="_blank" rel="external">https://arxiv.org/abs/1703.03054</a></li>
</ul>
<p><strong>Detecting Visual Relationships with Deep Relational Networks</strong></p>
<ul>
<li>intro: CVPR 2017 oral. The Chinese University of Hong Kong</li>
<li>arxiv: <a href="https://arxiv.org/abs/1704.03114" target="_blank" rel="external">https://arxiv.org/abs/1704.03114</a></li>
</ul>
<p><strong>Identifying Spatial Relations in Images using Convolutional Neural Networks</strong></p>
<p><a href="https://arxiv.org/abs/1706.04215" target="_blank" rel="external">https://arxiv.org/abs/1706.04215</a></p>
<p><strong>PPR-FCN: Weakly Supervised Visual Relation Detection via Parallel Pairwise R-FCN</strong></p>
<ul>
<li>intro: ICCV</li>
<li>arxiv: <a href="https://arxiv.org/abs/1708.01956" target="_blank" rel="external">https://arxiv.org/abs/1708.01956</a></li>
</ul>
<p><strong>Natural Language Guided Visual Relationship Detection</strong></p>
<p><a href="https://arxiv.org/abs/1711.06032" target="_blank" rel="external">https://arxiv.org/abs/1711.06032</a></p>
<h1 id="Face-Deteciton"><a href="#Face-Deteciton" class="headerlink" title="Face Deteciton"></a>Face Deteciton</h1><p><strong>Multi-view Face Detection Using Deep Convolutional Neural Networks</strong></p>
<ul>
<li>intro: Yahoo</li>
<li>arxiv: <a href="http://arxiv.org/abs/1502.02766" target="_blank" rel="external">http://arxiv.org/abs/1502.02766</a></li>
<li>github: <a href="https://github.com/guoyilin/FaceDetection_CNN" target="_blank" rel="external">https://github.com/guoyilin/FaceDetection_CNN</a></li>
</ul>
<p><strong>From Facial Parts Responses to Face Detection: A Deep Learning Approach</strong></p>
<p><img src="http://personal.ie.cuhk.edu.hk/~ys014/projects/Faceness/support/index.png" alt=""></p>
<ul>
<li>intro: ICCV 2015. CUHK</li>
<li>project page: <a href="http://personal.ie.cuhk.edu.hk/~ys014/projects/Faceness/Faceness.html" target="_blank" rel="external">http://personal.ie.cuhk.edu.hk/~ys014/projects/Faceness/Faceness.html</a></li>
<li>arxiv: <a href="https://arxiv.org/abs/1509.06451" target="_blank" rel="external">https://arxiv.org/abs/1509.06451</a></li>
<li>paper: <a href="http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Yang_From_Facial_Parts_ICCV_2015_paper.pdf" target="_blank" rel="external">http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Yang_From_Facial_Parts_ICCV_2015_paper.pdf</a></li>
</ul>
<p><strong>Compact Convolutional Neural Network Cascade for Face Detection</strong></p>
<ul>
<li>arxiv: <a href="http://arxiv.org/abs/1508.01292" target="_blank" rel="external">http://arxiv.org/abs/1508.01292</a></li>
<li>github: <a href="https://github.com/Bkmz21/FD-Evaluation" target="_blank" rel="external">https://github.com/Bkmz21/FD-Evaluation</a></li>
<li>github: <a href="https://github.com/Bkmz21/CompactCNNCascade" target="_blank" rel="external">https://github.com/Bkmz21/CompactCNNCascade</a></li>
</ul>
<p><strong>Face Detection with End-to-End Integration of a ConvNet and a 3D Model</strong></p>
<ul>
<li>intro: ECCV 2016</li>
<li>arxiv: <a href="https://arxiv.org/abs/1606.00850" target="_blank" rel="external">https://arxiv.org/abs/1606.00850</a></li>
<li>github(MXNet): <a href="https://github.com/tfwu/FaceDetection-ConvNet-3D" target="_blank" rel="external">https://github.com/tfwu/FaceDetection-ConvNet-3D</a></li>
</ul>
<p><strong>CMS-RCNN: Contextual Multi-Scale Region-based CNN for Unconstrained Face Detection</strong></p>
<ul>
<li>intro: CMU</li>
<li>arxiv: <a href="https://arxiv.org/abs/1606.05413" target="_blank" rel="external">https://arxiv.org/abs/1606.05413</a></li>
</ul>
<p><strong>Towards a Deep Learning Framework for Unconstrained Face Detection</strong></p>
<ul>
<li>intro: overlap with CMS-RCNN</li>
<li>arxiv: <a href="https://arxiv.org/abs/1612.05322" target="_blank" rel="external">https://arxiv.org/abs/1612.05322</a></li>
</ul>
<p><strong>Supervised Transformer Network for Efficient Face Detection</strong></p>
<ul>
<li>arxiv: <a href="http://arxiv.org/abs/1607.05477" target="_blank" rel="external">http://arxiv.org/abs/1607.05477</a></li>
</ul>
<p><strong>UnitBox: An Advanced Object Detection Network</strong></p>
<ul>
<li>intro: ACM MM 2016</li>
<li>keywords: IOULoss</li>
<li>arxiv: <a href="http://arxiv.org/abs/1608.01471" target="_blank" rel="external">http://arxiv.org/abs/1608.01471</a></li>
</ul>
<p><strong>Bootstrapping Face Detection with Hard Negative Examples</strong></p>
<ul>
<li>author: 万韶华 @ 小米.</li>
<li>intro: Faster R-CNN, hard negative mining. state-of-the-art on the FDDB dataset</li>
<li>arxiv: <a href="http://arxiv.org/abs/1608.02236" target="_blank" rel="external">http://arxiv.org/abs/1608.02236</a></li>
</ul>
<p><strong>Grid Loss: Detecting Occluded Faces</strong></p>
<ul>
<li>intro: ECCV 2016</li>
<li>arxiv: <a href="https://arxiv.org/abs/1609.00129" target="_blank" rel="external">https://arxiv.org/abs/1609.00129</a></li>
<li>paper: <a href="http://lrs.icg.tugraz.at/pubs/opitz_eccv_16.pdf" target="_blank" rel="external">http://lrs.icg.tugraz.at/pubs/opitz_eccv_16.pdf</a></li>
<li>poster: <a href="http://www.eccv2016.org/files/posters/P-2A-34.pdf" target="_blank" rel="external">http://www.eccv2016.org/files/posters/P-2A-34.pdf</a></li>
</ul>
<p><strong>A Multi-Scale Cascade Fully Convolutional Network Face Detector</strong></p>
<ul>
<li>intro: ICPR 2016</li>
<li>arxiv: <a href="http://arxiv.org/abs/1609.03536" target="_blank" rel="external">http://arxiv.org/abs/1609.03536</a></li>
</ul>
<h2 id="MTCNN"><a href="#MTCNN" class="headerlink" title="MTCNN"></a>MTCNN</h2><p><strong>Joint Face Detection and Alignment using Multi-task Cascaded Convolutional Networks</strong></p>
<p><strong>Joint Face Detection and Alignment using Multi-task Cascaded Convolutional Neural Networks</strong></p>
<p><img src="https://kpzhang93.github.io/MTCNN_face_detection_alignment/support/index.png" alt=""></p>
<ul>
<li>project page: <a href="https://kpzhang93.github.io/MTCNN_face_detection_alignment/index.html" target="_blank" rel="external">https://kpzhang93.github.io/MTCNN_face_detection_alignment/index.html</a></li>
<li>arxiv: <a href="https://arxiv.org/abs/1604.02878" target="_blank" rel="external">https://arxiv.org/abs/1604.02878</a></li>
<li>github(official, Matlab): <a href="https://github.com/kpzhang93/MTCNN_face_detection_alignment" target="_blank" rel="external">https://github.com/kpzhang93/MTCNN_face_detection_alignment</a></li>
<li>github: <a href="https://github.com/pangyupo/mxnet_mtcnn_face_detection" target="_blank" rel="external">https://github.com/pangyupo/mxnet_mtcnn_face_detection</a></li>
<li>github: <a href="https://github.com/DaFuCoding/MTCNN_Caffe" target="_blank" rel="external">https://github.com/DaFuCoding/MTCNN_Caffe</a></li>
<li>github(MXNet): <a href="https://github.com/Seanlinx/mtcnn" target="_blank" rel="external">https://github.com/Seanlinx/mtcnn</a></li>
<li>github: <a href="https://github.com/Pi-DeepLearning/RaspberryPi-FaceDetection-MTCNN-Caffe-With-Motion" target="_blank" rel="external">https://github.com/Pi-DeepLearning/RaspberryPi-FaceDetection-MTCNN-Caffe-With-Motion</a></li>
<li>github(Caffe): <a href="https://github.com/foreverYoungGitHub/MTCNN" target="_blank" rel="external">https://github.com/foreverYoungGitHub/MTCNN</a></li>
<li>github: <a href="https://github.com/CongWeilin/mtcnn-caffe" target="_blank" rel="external">https://github.com/CongWeilin/mtcnn-caffe</a></li>
<li>github(OpenCV+OpenBlas): <a href="https://github.com/AlphaQi/MTCNN-light" target="_blank" rel="external">https://github.com/AlphaQi/MTCNN-light</a></li>
<li>github(Tensorflow+golang): <a href="https://github.com/jdeng/goface" target="_blank" rel="external">https://github.com/jdeng/goface</a></li>
</ul>
<p><strong>Face Detection using Deep Learning: An Improved Faster RCNN Approach</strong></p>
<ul>
<li>intro: DeepIR Inc</li>
<li>arxiv: <a href="https://arxiv.org/abs/1701.08289" target="_blank" rel="external">https://arxiv.org/abs/1701.08289</a></li>
</ul>
<p><strong>Faceness-Net: Face Detection through Deep Facial Part Responses</strong></p>
<ul>
<li>intro: An extended version of ICCV 2015 paper</li>
<li>arxiv: <a href="https://arxiv.org/abs/1701.08393" target="_blank" rel="external">https://arxiv.org/abs/1701.08393</a></li>
</ul>
<p><strong>Multi-Path Region-Based Convolutional Neural Network for Accurate Detection of Unconstrained “Hard Faces”</strong></p>
<ul>
<li>intro: CVPR 2017. MP-RCNN, MP-RPN</li>
<li>arxiv: <a href="https://arxiv.org/abs/1703.09145" target="_blank" rel="external">https://arxiv.org/abs/1703.09145</a></li>
</ul>
<p><strong>End-To-End Face Detection and Recognition</strong></p>
<p><a href="https://arxiv.org/abs/1703.10818" target="_blank" rel="external">https://arxiv.org/abs/1703.10818</a></p>
<p><strong>Face R-CNN</strong></p>
<p><a href="https://arxiv.org/abs/1706.01061" target="_blank" rel="external">https://arxiv.org/abs/1706.01061</a></p>
<p><strong>Face Detection through Scale-Friendly Deep Convolutional Networks</strong></p>
<p><a href="https://arxiv.org/abs/1706.02863" target="_blank" rel="external">https://arxiv.org/abs/1706.02863</a></p>
<p><strong>Scale-Aware Face Detection</strong></p>
<ul>
<li>intro: CVPR 2017. SenseTime &amp; Tsinghua University</li>
<li>arxiv: <a href="https://arxiv.org/abs/1706.09876" target="_blank" rel="external">https://arxiv.org/abs/1706.09876</a></li>
</ul>
<p><strong>Multi-Branch Fully Convolutional Network for Face Detection</strong></p>
<p><a href="https://arxiv.org/abs/1707.06330" target="_blank" rel="external">https://arxiv.org/abs/1707.06330</a></p>
<p><strong>SSH: Single Stage Headless Face Detector</strong></p>
<ul>
<li>intro: ICCV 2017. University of Maryland</li>
<li>arxiv: <a href="https://arxiv.org/abs/1708.03979" target="_blank" rel="external">https://arxiv.org/abs/1708.03979</a></li>
<li>github(official, Caffe): <a href="https://github.com/mahyarnajibi/SSH" target="_blank" rel="external">https://github.com/mahyarnajibi/SSH</a></li>
</ul>
<p><strong>Dockerface: an easy to install and use Faster R-CNN face detector in a Docker container</strong></p>
<p><a href="https://arxiv.org/abs/1708.04370" target="_blank" rel="external">https://arxiv.org/abs/1708.04370</a></p>
<p><strong>FaceBoxes: A CPU Real-time Face Detector with High Accuracy</strong></p>
<ul>
<li>intro: IJCB 2017</li>
<li>keywords: Rapidly Digested Convolutional Layers (RDCL), Multiple Scale Convolutional Layers (MSCL)</li>
<li>intro: the proposed detector runs at 20 FPS on a single CPU core and 125 FPS using a GPU for VGA-resolution images</li>
<li>arxiv: <a href="https://arxiv.org/abs/1708.05234" target="_blank" rel="external">https://arxiv.org/abs/1708.05234</a></li>
</ul>
<p><strong>S3FD: Single Shot Scale-invariant Face Detector</strong></p>
<ul>
<li>intro: ICCV 2017. Chinese Academy of Sciences</li>
<li>intro: can run at 36 FPS on a Nvidia Titan X (Pascal) for VGA-resolution images</li>
<li>arxiv: <a href="https://arxiv.org/abs/1708.05237" target="_blank" rel="external">https://arxiv.org/abs/1708.05237</a></li>
<li>github: <a href="https://github.com//clcarwin/SFD_pytorch" target="_blank" rel="external">https://github.com//clcarwin/SFD_pytorch</a></li>
</ul>
<p><strong>Detecting Faces Using Region-based Fully Convolutional Networks</strong></p>
<p><a href="https://arxiv.org/abs/1709.05256" target="_blank" rel="external">https://arxiv.org/abs/1709.05256</a></p>
<p><strong>AffordanceNet: An End-to-End Deep Learning Approach for Object Affordance Detection</strong></p>
<p><a href="https://arxiv.org/abs/1709.07326" target="_blank" rel="external">https://arxiv.org/abs/1709.07326</a></p>
<p><strong>Face Attention Network: An effective Face Detector for the Occluded Faces</strong></p>
<p><a href="https://arxiv.org/abs/1711.07246" target="_blank" rel="external">https://arxiv.org/abs/1711.07246</a></p>
<p><strong>Feature Agglomeration Networks for Single Stage Face Detection</strong></p>
<p><a href="https://arxiv.org/abs/1712.00721" target="_blank" rel="external">https://arxiv.org/abs/1712.00721</a></p>
<p><strong>Face Detection Using Improved Faster RCNN</strong></p>
<ul>
<li>intro: Huawei Cloud BU</li>
<li>arxiv: <a href="https://arxiv.org/abs/1802.02142" target="_blank" rel="external">https://arxiv.org/abs/1802.02142</a></li>
</ul>
<p><strong>PyramidBox: A Context-assisted Single Shot Face Detector</strong></p>
<ul>
<li>intro: Baidu, Inc</li>
<li>arxiv: <a href="https://arxiv.org/abs/1803.07737" target="_blank" rel="external">https://arxiv.org/abs/1803.07737</a></li>
</ul>
<h2 id="Detect-Small-Faces"><a href="#Detect-Small-Faces" class="headerlink" title="Detect Small Faces"></a>Detect Small Faces</h2><p><strong>Finding Tiny Faces</strong></p>
<ul>
<li>intro: CVPR 2017. CMU</li>
<li>project page: <a href="http://www.cs.cmu.edu/~peiyunh/tiny/index.html" target="_blank" rel="external">http://www.cs.cmu.edu/~peiyunh/tiny/index.html</a></li>
<li>arxiv: <a href="https://arxiv.org/abs/1612.04402" target="_blank" rel="external">https://arxiv.org/abs/1612.04402</a></li>
<li>github(official, Matlab): <a href="https://github.com/peiyunh/tiny" target="_blank" rel="external">https://github.com/peiyunh/tiny</a></li>
<li>github(inference-only): <a href="https://github.com/chinakook/hr101_mxnet" target="_blank" rel="external">https://github.com/chinakook/hr101_mxnet</a></li>
<li>github: <a href="https://github.com/cydonia999/Tiny_Faces_in_Tensorflow" target="_blank" rel="external">https://github.com/cydonia999/Tiny_Faces_in_Tensorflow</a></li>
</ul>
<p><strong>Detecting and counting tiny faces</strong></p>
<ul>
<li>intro: ENS Paris-Saclay. ExtendedTinyFaces</li>
<li>intro: Detecting and counting small objects - Analysis, review and application to counting</li>
<li>arxiv: <a href="https://arxiv.org/abs/1801.06504" target="_blank" rel="external">https://arxiv.org/abs/1801.06504</a></li>
<li>github: <a href="https://github.com/alexattia/ExtendedTinyFaces" target="_blank" rel="external">https://github.com/alexattia/ExtendedTinyFaces</a></li>
</ul>
<p><strong>Seeing Small Faces from Robust Anchor’s Perspective</strong></p>
<ul>
<li>intro: CVPR 2018</li>
<li>arxiv: <a href="https://arxiv.org/abs/1802.09058" target="_blank" rel="external">https://arxiv.org/abs/1802.09058</a></li>
</ul>
<p><strong>Face-MagNet: Magnifying Feature Maps to Detect Small Faces</strong></p>
<ul>
<li>intro: WACV 2018</li>
<li>keywords: Face Magnifier Network (Face-MageNet)</li>
<li>arxiv: <a href="https://arxiv.org/abs/1803.05258" target="_blank" rel="external">https://arxiv.org/abs/1803.05258</a></li>
<li>github: <a href="https://github.com/po0ya/face-magnet" target="_blank" rel="external">https://github.com/po0ya/face-magnet</a></li>
</ul>
<h1 id="Person-Head-Detection"><a href="#Person-Head-Detection" class="headerlink" title="Person Head Detection"></a>Person Head Detection</h1><p><strong>Context-aware CNNs for person head detection</strong></p>
<ul>
<li>intro: ICCV 2015</li>
<li>project page: <a href="http://www.di.ens.fr/willow/research/headdetection/" target="_blank" rel="external">http://www.di.ens.fr/willow/research/headdetection/</a></li>
<li>arxiv: <a href="http://arxiv.org/abs/1511.07917" target="_blank" rel="external">http://arxiv.org/abs/1511.07917</a></li>
<li>github: <a href="https://github.com/aosokin/cnn_head_detection" target="_blank" rel="external">https://github.com/aosokin/cnn_head_detection</a></li>
</ul>
<h1 id="Pedestrian-Detection-People-Detection"><a href="#Pedestrian-Detection-People-Detection" class="headerlink" title="Pedestrian Detection / People Detection"></a>Pedestrian Detection / People Detection</h1><p><strong>Pedestrian Detection aided by Deep Learning Semantic Tasks</strong></p>
<ul>
<li>intro: CVPR 2015</li>
<li>project page: <a href="http://mmlab.ie.cuhk.edu.hk/projects/TA-CNN/" target="_blank" rel="external">http://mmlab.ie.cuhk.edu.hk/projects/TA-CNN/</a></li>
<li>arxiv: <a href="http://arxiv.org/abs/1412.0069" target="_blank" rel="external">http://arxiv.org/abs/1412.0069</a></li>
</ul>
<p><strong>Deep Learning Strong Parts for Pedestrian Detection</strong></p>
<ul>
<li>intro: ICCV 2015. CUHK. DeepParts</li>
<li>intro: Achieving 11.89% average miss rate on Caltech Pedestrian Dataset</li>
<li>paper: <a href="http://personal.ie.cuhk.edu.hk/~pluo/pdf/tianLWTiccv15.pdf" target="_blank" rel="external">http://personal.ie.cuhk.edu.hk/~pluo/pdf/tianLWTiccv15.pdf</a></li>
</ul>
<p><strong>Taking a Deeper Look at Pedestrians</strong></p>
<ul>
<li>intro: CVPR 2015</li>
<li>arxiv: <a href="https://arxiv.org/abs/1501.05790" target="_blank" rel="external">https://arxiv.org/abs/1501.05790</a></li>
</ul>
<p><strong>Convolutional Channel Features</strong></p>
<ul>
<li>intro: ICCV 2015</li>
<li>arxiv: <a href="https://arxiv.org/abs/1504.07339" target="_blank" rel="external">https://arxiv.org/abs/1504.07339</a></li>
<li>github: <a href="https://github.com/byangderek/CCF" target="_blank" rel="external">https://github.com/byangderek/CCF</a></li>
</ul>
<p><strong>End-to-end people detection in crowded scenes</strong></p>
<p><img src="/assets/object-detection-materials/end_to_end_people_detection_in_crowded_scenes.jpg" alt=""></p>
<ul>
<li>arxiv: <a href="http://arxiv.org/abs/1506.04878" target="_blank" rel="external">http://arxiv.org/abs/1506.04878</a></li>
<li>github: <a href="https://github.com/Russell91/reinspect" target="_blank" rel="external">https://github.com/Russell91/reinspect</a></li>
<li>ipn: <a href="http://nbviewer.ipython.org/github/Russell91/ReInspect/blob/master/evaluation_reinspect.ipynb" target="_blank" rel="external">http://nbviewer.ipython.org/github/Russell91/ReInspect/blob/master/evaluation_reinspect.ipynb</a></li>
<li>youtube: <a href="https://www.youtube.com/watch?v=QeWl0h3kQ24" target="_blank" rel="external">https://www.youtube.com/watch?v=QeWl0h3kQ24</a></li>
</ul>
<p><strong>Learning Complexity-Aware Cascades for Deep Pedestrian Detection</strong></p>
<ul>
<li>intro: ICCV 2015</li>
<li>arxiv: <a href="https://arxiv.org/abs/1507.05348" target="_blank" rel="external">https://arxiv.org/abs/1507.05348</a></li>
</ul>
<p><strong>Deep convolutional neural networks for pedestrian detection</strong></p>
<ul>
<li>arxiv: <a href="http://arxiv.org/abs/1510.03608" target="_blank" rel="external">http://arxiv.org/abs/1510.03608</a></li>
<li>github: <a href="https://github.com/DenisTome/DeepPed" target="_blank" rel="external">https://github.com/DenisTome/DeepPed</a></li>
</ul>
<p><strong>Scale-aware Fast R-CNN for Pedestrian Detection</strong></p>
<ul>
<li>arxiv: <a href="https://arxiv.org/abs/1510.08160" target="_blank" rel="external">https://arxiv.org/abs/1510.08160</a></li>
</ul>
<p><strong>New algorithm improves speed and accuracy of pedestrian detection</strong></p>
<ul>
<li>blog: <a href="http://www.eurekalert.org/pub_releases/2016-02/uoc--nai020516.php" target="_blank" rel="external">http://www.eurekalert.org/pub_releases/2016-02/uoc–nai020516.php</a></li>
</ul>
<p><strong>Pushing the Limits of Deep CNNs for Pedestrian Detection</strong></p>
<ul>
<li>intro: “set a new record on the Caltech pedestrian dataset, lowering the log-average miss rate from 11.7% to 8.9%”</li>
<li>arxiv: <a href="http://arxiv.org/abs/1603.04525" target="_blank" rel="external">http://arxiv.org/abs/1603.04525</a></li>
</ul>
<p><strong>A Real-Time Deep Learning Pedestrian Detector for Robot Navigation</strong></p>
<ul>
<li>arxiv: <a href="http://arxiv.org/abs/1607.04436" target="_blank" rel="external">http://arxiv.org/abs/1607.04436</a></li>
</ul>
<p><strong>A Real-Time Pedestrian Detector using Deep Learning for Human-Aware Navigation</strong></p>
<ul>
<li>arxiv: <a href="http://arxiv.org/abs/1607.04441" target="_blank" rel="external">http://arxiv.org/abs/1607.04441</a></li>
</ul>
<p><strong>Is Faster R-CNN Doing Well for Pedestrian Detection?</strong></p>
<ul>
<li>intro: ECCV 2016</li>
<li>arxiv: <a href="http://arxiv.org/abs/1607.07032" target="_blank" rel="external">http://arxiv.org/abs/1607.07032</a></li>
<li>github: <a href="https://github.com/zhangliliang/RPN_BF/tree/RPN-pedestrian" target="_blank" rel="external">https://github.com/zhangliliang/RPN_BF/tree/RPN-pedestrian</a></li>
</ul>
<p><strong>Unsupervised Deep Domain Adaptation for Pedestrian Detection</strong></p>
<ul>
<li>intro: ECCV Workshop 2016</li>
<li>arxiv: <a href="https://arxiv.org/abs/1802.03269" target="_blank" rel="external">https://arxiv.org/abs/1802.03269</a></li>
</ul>
<p><strong>Reduced Memory Region Based Deep Convolutional Neural Network Detection</strong></p>
<ul>
<li>intro: IEEE 2016 ICCE-Berlin</li>
<li>arxiv: <a href="http://arxiv.org/abs/1609.02500" target="_blank" rel="external">http://arxiv.org/abs/1609.02500</a></li>
</ul>
<p><strong>Fused DNN: A deep neural network fusion approach to fast and robust pedestrian detection</strong></p>
<ul>
<li>arxiv: <a href="https://arxiv.org/abs/1610.03466" target="_blank" rel="external">https://arxiv.org/abs/1610.03466</a></li>
</ul>
<p><strong>Detecting People in Artwork with CNNs</strong></p>
<ul>
<li>intro: ECCV 2016 Workshops</li>
<li>arxiv: <a href="https://arxiv.org/abs/1610.08871" target="_blank" rel="external">https://arxiv.org/abs/1610.08871</a></li>
</ul>
<p><strong>Multispectral Deep Neural Networks for Pedestrian Detection</strong></p>
<ul>
<li>intro: BMVC 2016 oral</li>
<li>arxiv: <a href="https://arxiv.org/abs/1611.02644" target="_blank" rel="external">https://arxiv.org/abs/1611.02644</a></li>
</ul>
<p><strong>Deep Multi-camera People Detection</strong></p>
<ul>
<li>arxiv: <a href="https://arxiv.org/abs/1702.04593" target="_blank" rel="external">https://arxiv.org/abs/1702.04593</a></li>
</ul>
<p><strong>Expecting the Unexpected: Training Detectors for Unusual Pedestrians with Adversarial Imposters</strong></p>
<ul>
<li>intro: CVPR 2017</li>
<li>project page: <a href="http://ml.cs.tsinghua.edu.cn:5000/publications/synunity/" target="_blank" rel="external">http://ml.cs.tsinghua.edu.cn:5000/publications/synunity/</a></li>
<li>arxiv: <a href="https://arxiv.org/abs/1703.06283" target="_blank" rel="external">https://arxiv.org/abs/1703.06283</a></li>
<li>github(Tensorflow): <a href="https://github.com/huangshiyu13/RPNplus" target="_blank" rel="external">https://github.com/huangshiyu13/RPNplus</a></li>
</ul>
<p><strong>Illuminating Pedestrians via Simultaneous Detection &amp; Segmentation</strong></p>
<p>[<a href="https://arxiv.org/abs/1706.08564](https://arxiv.org/abs/1706.08564" target="_blank" rel="external">https://arxiv.org/abs/1706.08564](https://arxiv.org/abs/1706.08564</a></p>
<p><strong>Rotational Rectification Network for Robust Pedestrian Detection</strong></p>
<ul>
<li>intro: CMU &amp; Volvo Construction</li>
<li>arxiv: <a href="https://arxiv.org/abs/1706.08917" target="_blank" rel="external">https://arxiv.org/abs/1706.08917</a></li>
</ul>
<p><strong>STD-PD: Generating Synthetic Training Data for Pedestrian Detection in Unannotated Videos</strong></p>
<ul>
<li>intro: The University of North Carolina at Chapel Hill</li>
<li>arxiv: <a href="https://arxiv.org/abs/1707.09100" target="_blank" rel="external">https://arxiv.org/abs/1707.09100</a></li>
</ul>
<p><strong>Too Far to See? Not Really! — Pedestrian Detection with Scale-aware Localization Policy</strong></p>
<p><a href="https://arxiv.org/abs/1709.00235" target="_blank" rel="external">https://arxiv.org/abs/1709.00235</a></p>
<p><strong>Repulsion Loss: Detecting Pedestrians in a Crowd</strong></p>
<p><a href="https://arxiv.org/abs/1711.07752" target="_blank" rel="external">https://arxiv.org/abs/1711.07752</a></p>
<p><strong>Aggregated Channels Network for Real-Time Pedestrian Detection</strong></p>
<p><a href="https://arxiv.org/abs/1801.00476" target="_blank" rel="external">https://arxiv.org/abs/1801.00476</a></p>
<p><strong>Illumination-aware Faster R-CNN for Robust Multispectral Pedestrian Detection</strong></p>
<ul>
<li>intro: State Key Lab of CAD&amp;CG, Zhejiang University</li>
<li>arxiv: <a href="https://arxiv.org/abs/1803.05347" target="_blank" rel="external">https://arxiv.org/abs/1803.05347</a></li>
</ul>
<h1 id="Vehicle-Detection"><a href="#Vehicle-Detection" class="headerlink" title="Vehicle Detection"></a>Vehicle Detection</h1><p><strong>DAVE: A Unified Framework for Fast Vehicle Detection and Annotation</strong></p>
<ul>
<li>intro: ECCV 2016</li>
<li>arxiv: <a href="http://arxiv.org/abs/1607.04564" target="_blank" rel="external">http://arxiv.org/abs/1607.04564</a></li>
</ul>
<p><strong>Evolving Boxes for fast Vehicle Detection</strong></p>
<ul>
<li>arxiv: <a href="https://arxiv.org/abs/1702.00254" target="_blank" rel="external">https://arxiv.org/abs/1702.00254</a></li>
</ul>
<p><strong>Fine-Grained Car Detection for Visual Census Estimation</strong></p>
<ul>
<li>intro: AAAI 2016</li>
<li>arxiv: <a href="https://arxiv.org/abs/1709.02480" target="_blank" rel="external">https://arxiv.org/abs/1709.02480</a></li>
</ul>
<h1 id="Traffic-Sign-Detection"><a href="#Traffic-Sign-Detection" class="headerlink" title="Traffic-Sign Detection"></a>Traffic-Sign Detection</h1><p><strong>Traffic-Sign Detection and Classification in the Wild</strong></p>
<ul>
<li>project page(code+dataset): <a href="http://cg.cs.tsinghua.edu.cn/traffic-sign/" target="_blank" rel="external">http://cg.cs.tsinghua.edu.cn/traffic-sign/</a></li>
<li>paper: <a href="http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Zhu_Traffic-Sign_Detection_and_CVPR_2016_paper.pdf" target="_blank" rel="external">http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Zhu_Traffic-Sign_Detection_and_CVPR_2016_paper.pdf</a></li>
<li>code &amp; model: <a href="http://cg.cs.tsinghua.edu.cn/traffic-sign/data_model_code/newdata0411.zip" target="_blank" rel="external">http://cg.cs.tsinghua.edu.cn/traffic-sign/data_model_code/newdata0411.zip</a></li>
</ul>
<p><strong>Detecting Small Signs from Large Images</strong></p>
<ul>
<li>intro: IEEE Conference on Information Reuse and Integration (IRI) 2017 oral</li>
<li>arxiv: <a href="https://arxiv.org/abs/1706.08574" target="_blank" rel="external">https://arxiv.org/abs/1706.08574</a></li>
</ul>
<h1 id="Skeleton-Detection"><a href="#Skeleton-Detection" class="headerlink" title="Skeleton Detection"></a>Skeleton Detection</h1><p><strong>Object Skeleton Extraction in Natural Images by Fusing Scale-associated Deep Side Outputs</strong></p>
<p><img src="https://camo.githubusercontent.com/88a65f132aa4ae4b0477e3ad02c13cdc498377d9/687474703a2f2f37786e37777a2e636f6d312e7a302e676c622e636c6f7564646e2e636f6d2f44656570536b656c65746f6e2e706e673f696d61676556696577322f322f772f353030" alt=""></p>
<ul>
<li>arxiv: <a href="http://arxiv.org/abs/1603.09446" target="_blank" rel="external">http://arxiv.org/abs/1603.09446</a></li>
<li>github: <a href="https://github.com/zeakey/DeepSkeleton" target="_blank" rel="external">https://github.com/zeakey/DeepSkeleton</a></li>
</ul>
<p><strong>DeepSkeleton: Learning Multi-task Scale-associated Deep Side Outputs for Object Skeleton Extraction in Natural Images</strong></p>
<ul>
<li>arxiv: <a href="http://arxiv.org/abs/1609.03659" target="_blank" rel="external">http://arxiv.org/abs/1609.03659</a></li>
</ul>
<p><strong>SRN: Side-output Residual Network for Object Symmetry Detection in the Wild</strong></p>
<ul>
<li>intro: CVPR 2017</li>
<li>arxiv: <a href="https://arxiv.org/abs/1703.02243" target="_blank" rel="external">https://arxiv.org/abs/1703.02243</a></li>
<li>github: <a href="https://github.com/KevinKecc/SRN" target="_blank" rel="external">https://github.com/KevinKecc/SRN</a></li>
</ul>
<p><strong>Hi-Fi: Hierarchical Feature Integration for Skeleton Detection</strong></p>
<p><a href="https://arxiv.org/abs/1801.01849" target="_blank" rel="external">https://arxiv.org/abs/1801.01849</a></p>
<h1 id="Fruit-Detection"><a href="#Fruit-Detection" class="headerlink" title="Fruit Detection"></a>Fruit Detection</h1><p><strong>Deep Fruit Detection in Orchards</strong></p>
<ul>
<li>arxiv: <a href="https://arxiv.org/abs/1610.03677" target="_blank" rel="external">https://arxiv.org/abs/1610.03677</a></li>
</ul>
<p><strong>Image Segmentation for Fruit Detection and Yield Estimation in Apple Orchards</strong></p>
<ul>
<li>intro: The Journal of Field Robotics in May 2016</li>
<li>project page: <a href="http://confluence.acfr.usyd.edu.au/display/AGPub/" target="_blank" rel="external">http://confluence.acfr.usyd.edu.au/display/AGPub/</a></li>
<li>arxiv: <a href="https://arxiv.org/abs/1610.08120" target="_blank" rel="external">https://arxiv.org/abs/1610.08120</a></li>
</ul>
<h2 id="Shadow-Detection"><a href="#Shadow-Detection" class="headerlink" title="Shadow Detection"></a>Shadow Detection</h2><p><strong>Fast Shadow Detection from a Single Image Using a Patched Convolutional Neural Network</strong></p>
<p><a href="https://arxiv.org/abs/1709.09283" target="_blank" rel="external">https://arxiv.org/abs/1709.09283</a></p>
<p><strong>A+D-Net: Shadow Detection with Adversarial Shadow Attenuation</strong></p>
<p><a href="https://arxiv.org/abs/1712.01361" target="_blank" rel="external">https://arxiv.org/abs/1712.01361</a></p>
<p><strong>Stacked Conditional Generative Adversarial Networks for Jointly Learning Shadow Detection and Shadow Removal</strong></p>
<p><a href="https://arxiv.org/abs/1712.02478" target="_blank" rel="external">https://arxiv.org/abs/1712.02478</a></p>
<p><strong>Direction-aware Spatial Context Features for Shadow Detection</strong></p>
<p><a href="https://arxiv.org/abs/1712.04142" target="_blank" rel="external">https://arxiv.org/abs/1712.04142</a></p>
<h1 id="Others-Deteciton"><a href="#Others-Deteciton" class="headerlink" title="Others Deteciton"></a>Others Deteciton</h1><p><strong>Deep Deformation Network for Object Landmark Localization</strong></p>
<ul>
<li>arxiv: <a href="http://arxiv.org/abs/1605.01014" target="_blank" rel="external">http://arxiv.org/abs/1605.01014</a></li>
</ul>
<p><strong>Fashion Landmark Detection in the Wild</strong></p>
<ul>
<li>intro: ECCV 2016</li>
<li>project page: <a href="http://personal.ie.cuhk.edu.hk/~lz013/projects/FashionLandmarks.html" target="_blank" rel="external">http://personal.ie.cuhk.edu.hk/~lz013/projects/FashionLandmarks.html</a></li>
<li>arxiv: <a href="http://arxiv.org/abs/1608.03049" target="_blank" rel="external">http://arxiv.org/abs/1608.03049</a></li>
<li>github(Caffe): <a href="https://github.com/liuziwei7/fashion-landmarks" target="_blank" rel="external">https://github.com/liuziwei7/fashion-landmarks</a></li>
</ul>
<p><strong>Deep Learning for Fast and Accurate Fashion Item Detection</strong></p>
<ul>
<li>intro: Kuznech Inc.</li>
<li>intro: MultiBox and Fast R-CNN</li>
<li>paper: <a href="https://kddfashion2016.mybluemix.net/kddfashion_finalSubmissions/Deep%20Learning%20for%20Fast%20and%20Accurate%20Fashion%20Item%20Detection.pdf" target="_blank" rel="external">https://kddfashion2016.mybluemix.net/kddfashion_finalSubmissions/Deep%20Learning%20for%20Fast%20and%20Accurate%20Fashion%20Item%20Detection.pdf</a></li>
</ul>
<p><strong>OSMDeepOD - OSM and Deep Learning based Object Detection from Aerial Imagery (formerly known as “OSM-Crosswalk-Detection”)</strong></p>
<p><img src="https://raw.githubusercontent.com/geometalab/OSMDeepOD/master/imgs/process.png" alt=""></p>
<ul>
<li>github: <a href="https://github.com/geometalab/OSMDeepOD" target="_blank" rel="external">https://github.com/geometalab/OSMDeepOD</a></li>
</ul>
<p><strong>Selfie Detection by Synergy-Constraint Based Convolutional Neural Network</strong></p>
<ul>
<li>intro:  IEEE SITIS 2016</li>
<li>arxiv: <a href="https://arxiv.org/abs/1611.04357" target="_blank" rel="external">https://arxiv.org/abs/1611.04357</a></li>
</ul>
<p><strong>Associative Embedding:End-to-End Learning for Joint Detection and Grouping</strong></p>
<ul>
<li>arxiv: <a href="https://arxiv.org/abs/1611.05424" target="_blank" rel="external">https://arxiv.org/abs/1611.05424</a></li>
</ul>
<p><strong>Deep Cuboid Detection: Beyond 2D Bounding Boxes</strong></p>
<ul>
<li>intro: CMU &amp; Magic Leap</li>
<li>arxiv: <a href="https://arxiv.org/abs/1611.10010" target="_blank" rel="external">https://arxiv.org/abs/1611.10010</a></li>
</ul>
<p><strong>Automatic Model Based Dataset Generation for Fast and Accurate Crop and Weeds Detection</strong></p>
<ul>
<li>arxiv: <a href="https://arxiv.org/abs/1612.03019" target="_blank" rel="external">https://arxiv.org/abs/1612.03019</a></li>
</ul>
<p><strong>Deep Learning Logo Detection with Data Expansion by Synthesising Context</strong></p>
<ul>
<li>arxiv: <a href="https://arxiv.org/abs/1612.09322" target="_blank" rel="external">https://arxiv.org/abs/1612.09322</a></li>
</ul>
<p><strong>Pixel-wise Ear Detection with Convolutional Encoder-Decoder Networks</strong></p>
<ul>
<li>arxiv: <a href="https://arxiv.org/abs/1702.00307" target="_blank" rel="external">https://arxiv.org/abs/1702.00307</a></li>
</ul>
<p><strong>Automatic Handgun Detection Alarm in Videos Using Deep Learning</strong></p>
<ul>
<li>arxiv: <a href="https://arxiv.org/abs/1702.05147" target="_blank" rel="external">https://arxiv.org/abs/1702.05147</a></li>
<li>results: <a href="https://github.com/SihamTabik/Pistol-Detection-in-Videos" target="_blank" rel="external">https://github.com/SihamTabik/Pistol-Detection-in-Videos</a></li>
</ul>
<p><strong>Objects as context for part detection</strong></p>
<p><a href="https://arxiv.org/abs/1703.09529" target="_blank" rel="external">https://arxiv.org/abs/1703.09529</a></p>
<p><strong>Using Deep Networks for Drone Detection</strong></p>
<ul>
<li>intro: AVSS 2017</li>
<li>arxiv: <a href="https://arxiv.org/abs/1706.05726" target="_blank" rel="external">https://arxiv.org/abs/1706.05726</a></li>
</ul>
<p><strong>Cut, Paste and Learn: Surprisingly Easy Synthesis for Instance Detection</strong></p>
<ul>
<li>intro: ICCV 2017</li>
<li>arxiv: <a href="https://arxiv.org/abs/1708.01642" target="_blank" rel="external">https://arxiv.org/abs/1708.01642</a></li>
</ul>
<p><strong>Target Driven Instance Detection</strong></p>
<p><a href="https://arxiv.org/abs/1803.04610" target="_blank" rel="external">https://arxiv.org/abs/1803.04610</a></p>
<p><strong>DeepVoting: An Explainable Framework for Semantic Part Detection under Partial Occlusion</strong></p>
<p><a href="https://arxiv.org/abs/1709.04577" target="_blank" rel="external">https://arxiv.org/abs/1709.04577</a></p>
<p><strong>VPGNet: Vanishing Point Guided Network for Lane and Road Marking Detection and Recognition</strong></p>
<ul>
<li>intro: ICCV 2017</li>
<li>arxiv: <a href="https://arxiv.org/abs/1710.06288" target="_blank" rel="external">https://arxiv.org/abs/1710.06288</a></li>
<li>github: <a href="https://github.com/SeokjuLee/VPGNet" target="_blank" rel="external">https://github.com/SeokjuLee/VPGNet</a></li>
</ul>
<p><strong>Grab, Pay and Eat: Semantic Food Detection for Smart Restaurants</strong></p>
<p><a href="https://arxiv.org/abs/1711.05128" target="_blank" rel="external">https://arxiv.org/abs/1711.05128</a></p>
<p><strong>ReMotENet: Efficient Relevant Motion Event Detection for Large-scale Home Surveillance Videos</strong></p>
<ul>
<li>intro: WACV 2018</li>
<li>arxiv: <a href="https://arxiv.org/abs/1801.02031" target="_blank" rel="external">https://arxiv.org/abs/1801.02031</a></li>
</ul>
<h1 id="Object-Proposal"><a href="#Object-Proposal" class="headerlink" title="Object Proposal"></a>Object Proposal</h1><p><strong>DeepProposal: Hunting Objects by Cascading Deep Convolutional Layers</strong></p>
<ul>
<li>arxiv: <a href="http://arxiv.org/abs/1510.04445" target="_blank" rel="external">http://arxiv.org/abs/1510.04445</a></li>
<li>github: <a href="https://github.com/aghodrati/deepproposal" target="_blank" rel="external">https://github.com/aghodrati/deepproposal</a></li>
</ul>
<p><strong>Scale-aware Pixel-wise Object Proposal Networks</strong></p>
<ul>
<li>intro: IEEE Transactions on Image Processing</li>
<li>arxiv: <a href="http://arxiv.org/abs/1601.04798" target="_blank" rel="external">http://arxiv.org/abs/1601.04798</a></li>
</ul>
<p><strong>Attend Refine Repeat: Active Box Proposal Generation via In-Out Localization</strong></p>
<ul>
<li>intro: BMVC 2016. AttractioNet</li>
<li>arxiv: <a href="https://arxiv.org/abs/1606.04446" target="_blank" rel="external">https://arxiv.org/abs/1606.04446</a></li>
<li>github: <a href="https://github.com/gidariss/AttractioNet" target="_blank" rel="external">https://github.com/gidariss/AttractioNet</a></li>
</ul>
<p><strong>Learning to Segment Object Proposals via Recursive Neural Networks</strong></p>
<ul>
<li>arxiv: <a href="https://arxiv.org/abs/1612.01057" target="_blank" rel="external">https://arxiv.org/abs/1612.01057</a></li>
</ul>
<p><strong>Learning Detection with Diverse Proposals</strong></p>
<ul>
<li>intro: CVPR 2017</li>
<li>keywords: differentiable Determinantal Point Process (DPP) layer, Learning Detection with Diverse Proposals (LDDP)</li>
<li>arxiv: <a href="https://arxiv.org/abs/1704.03533" target="_blank" rel="external">https://arxiv.org/abs/1704.03533</a></li>
</ul>
<p><strong>ScaleNet: Guiding Object Proposal Generation in Supermarkets and Beyond</strong></p>
<ul>
<li>keywords: product detection</li>
<li>arxiv: <a href="https://arxiv.org/abs/1704.06752" target="_blank" rel="external">https://arxiv.org/abs/1704.06752</a></li>
</ul>
<p><strong>Improving Small Object Proposals for Company Logo Detection</strong></p>
<ul>
<li>intro: ICMR 2017</li>
<li>arxiv: <a href="https://arxiv.org/abs/1704.08881" target="_blank" rel="external">https://arxiv.org/abs/1704.08881</a></li>
</ul>
<h1 id="Localization"><a href="#Localization" class="headerlink" title="Localization"></a>Localization</h1><p><strong>Beyond Bounding Boxes: Precise Localization of Objects in Images</strong></p>
<ul>
<li>intro: PhD Thesis</li>
<li>homepage: <a href="http://www.eecs.berkeley.edu/Pubs/TechRpts/2015/EECS-2015-193.html" target="_blank" rel="external">http://www.eecs.berkeley.edu/Pubs/TechRpts/2015/EECS-2015-193.html</a></li>
<li>phd-thesis: <a href="http://www.eecs.berkeley.edu/Pubs/TechRpts/2015/EECS-2015-193.pdf" target="_blank" rel="external">http://www.eecs.berkeley.edu/Pubs/TechRpts/2015/EECS-2015-193.pdf</a></li>
<li>github(“SDS using hypercolumns”): <a href="https://github.com/bharath272/sds" target="_blank" rel="external">https://github.com/bharath272/sds</a></li>
</ul>
<p><strong>Weakly Supervised Object Localization with Multi-fold Multiple Instance Learning</strong></p>
<ul>
<li>arxiv: <a href="http://arxiv.org/abs/1503.00949" target="_blank" rel="external">http://arxiv.org/abs/1503.00949</a></li>
</ul>
<p><strong>Weakly Supervised Object Localization Using Size Estimates</strong></p>
<ul>
<li>arxiv: <a href="http://arxiv.org/abs/1608.04314" target="_blank" rel="external">http://arxiv.org/abs/1608.04314</a></li>
</ul>
<p><strong>Active Object Localization with Deep Reinforcement Learning</strong></p>
<ul>
<li>intro: ICCV 2015</li>
<li>keywords: Markov Decision Process</li>
<li>arxiv: <a href="https://arxiv.org/abs/1511.06015" target="_blank" rel="external">https://arxiv.org/abs/1511.06015</a></li>
</ul>
<p><strong>Localizing objects using referring expressions</strong></p>
<ul>
<li>intro: ECCV 2016</li>
<li>keywords: LSTM, multiple instance learning (MIL)</li>
<li>paper: <a href="http://www.umiacs.umd.edu/~varun/files/refexp-ECCV16.pdf" target="_blank" rel="external">http://www.umiacs.umd.edu/~varun/files/refexp-ECCV16.pdf</a></li>
<li>github: <a href="https://github.com/varun-nagaraja/referring-expressions" target="_blank" rel="external">https://github.com/varun-nagaraja/referring-expressions</a></li>
</ul>
<p><strong>LocNet: Improving Localization Accuracy for Object Detection</strong></p>
<ul>
<li>intro: CVPR 2016 oral</li>
<li>arxiv: <a href="http://arxiv.org/abs/1511.07763" target="_blank" rel="external">http://arxiv.org/abs/1511.07763</a></li>
<li>github: <a href="https://github.com/gidariss/LocNet" target="_blank" rel="external">https://github.com/gidariss/LocNet</a></li>
</ul>
<p><strong>Learning Deep Features for Discriminative Localization</strong></p>
<p><img src="http://cnnlocalization.csail.mit.edu/framework.jpg" alt=""></p>
<ul>
<li>homepage: <a href="http://cnnlocalization.csail.mit.edu/" target="_blank" rel="external">http://cnnlocalization.csail.mit.edu/</a></li>
<li>arxiv: <a href="http://arxiv.org/abs/1512.04150" target="_blank" rel="external">http://arxiv.org/abs/1512.04150</a></li>
<li>github(Tensorflow): <a href="https://github.com/jazzsaxmafia/Weakly_detector" target="_blank" rel="external">https://github.com/jazzsaxmafia/Weakly_detector</a></li>
<li>github: <a href="https://github.com/metalbubble/CAM" target="_blank" rel="external">https://github.com/metalbubble/CAM</a></li>
<li>github: <a href="https://github.com/tdeboissiere/VGG16CAM-keras" target="_blank" rel="external">https://github.com/tdeboissiere/VGG16CAM-keras</a></li>
</ul>
<p><strong>ContextLocNet: Context-Aware Deep Network Models for Weakly Supervised Localization</strong></p>
<p><img src="http://www.di.ens.fr/willow/research/contextlocnet/model.png" alt=""></p>
<ul>
<li>intro: ECCV 2016</li>
<li>project page: <a href="http://www.di.ens.fr/willow/research/contextlocnet/" target="_blank" rel="external">http://www.di.ens.fr/willow/research/contextlocnet/</a></li>
<li>arxiv: <a href="http://arxiv.org/abs/1609.04331" target="_blank" rel="external">http://arxiv.org/abs/1609.04331</a></li>
<li>github: <a href="https://github.com/vadimkantorov/contextlocnet" target="_blank" rel="external">https://github.com/vadimkantorov/contextlocnet</a></li>
</ul>
<p><strong>Ensemble of Part Detectors for Simultaneous Classification and Localization</strong></p>
<p><a href="https://arxiv.org/abs/1705.10034" target="_blank" rel="external">https://arxiv.org/abs/1705.10034</a></p>
<p><strong>STNet: Selective Tuning of Convolutional Networks for Object Localization</strong></p>
<p><a href="https://arxiv.org/abs/1708.06418" target="_blank" rel="external">https://arxiv.org/abs/1708.06418</a></p>
<p><strong>Soft Proposal Networks for Weakly Supervised Object Localization</strong></p>
<ul>
<li>intro: ICCV 2017</li>
<li>arxiv: <a href="https://arxiv.org/abs/1709.01829" target="_blank" rel="external">https://arxiv.org/abs/1709.01829</a></li>
</ul>
<p><strong>Fine-grained Discriminative Localization via Saliency-guided Faster R-CNN</strong></p>
<ul>
<li>intro: ACM MM 2017</li>
<li>arxiv: <a href="https://arxiv.org/abs/1709.08295" target="_blank" rel="external">https://arxiv.org/abs/1709.08295</a></li>
</ul>
<h1 id="Tutorials-Talks"><a href="#Tutorials-Talks" class="headerlink" title="Tutorials / Talks"></a>Tutorials / Talks</h1><p><strong>Convolutional Feature Maps: Elements of efficient (and accurate) CNN-based object detection</strong></p>
<ul>
<li>slides: <a href="http://research.microsoft.com/en-us/um/people/kahe/iccv15tutorial/iccv2015_tutorial_convolutional_feature_maps_kaiminghe.pdf" target="_blank" rel="external">http://research.microsoft.com/en-us/um/people/kahe/iccv15tutorial/iccv2015_tutorial_convolutional_feature_maps_kaiminghe.pdf</a></li>
</ul>
<p><strong>Towards Good Practices for Recognition &amp; Detection</strong></p>
<ul>
<li>intro: Hikvision Research Institute. Supervised Data Augmentation (SDA)</li>
<li>slides: <a href="http://image-net.org/challenges/talks/2016/Hikvision_at_ImageNet_2016.pdf" target="_blank" rel="external">http://image-net.org/challenges/talks/2016/Hikvision_at_ImageNet_2016.pdf</a></li>
</ul>
<h1 id="Projects"><a href="#Projects" class="headerlink" title="Projects"></a>Projects</h1><p><strong>Detectron</strong></p>
<ul>
<li>intro: FAIR’s research platform for object detection research, implementing popular algorithms like Mask R-CNN and RetinaNet.</li>
<li>github: <a href="https://github.com/facebookresearch/Detectron" target="_blank" rel="external">https://github.com/facebookresearch/Detectron</a></li>
</ul>
<p><strong>TensorBox: a simple framework for training neural networks to detect objects in images</strong></p>
<ul>
<li>intro: “The basic model implements the simple and robust GoogLeNet-OverFeat algorithm.<br>We additionally provide an implementation of the <a href="https://github.com/Russell91/ReInspect/" target="_blank" rel="external">ReInspect</a> algorithm”</li>
<li>github: <a href="https://github.com/Russell91/TensorBox" target="_blank" rel="external">https://github.com/Russell91/TensorBox</a></li>
</ul>
<p><strong>Object detection in torch: Implementation of some object detection frameworks in torch</strong></p>
<ul>
<li>github: <a href="https://github.com/fmassa/object-detection.torch" target="_blank" rel="external">https://github.com/fmassa/object-detection.torch</a></li>
</ul>
<p><strong>Using DIGITS to train an Object Detection network</strong></p>
<ul>
<li>github: <a href="https://github.com/NVIDIA/DIGITS/blob/master/examples/object-detection/README.md" target="_blank" rel="external">https://github.com/NVIDIA/DIGITS/blob/master/examples/object-detection/README.md</a></li>
</ul>
<p><strong>FCN-MultiBox Detector</strong></p>
<ul>
<li>intro: Full convolution MultiBox Detector (like SSD) implemented in Torch.</li>
<li>github: <a href="https://github.com/teaonly/FMD.torch" target="_blank" rel="external">https://github.com/teaonly/FMD.torch</a></li>
</ul>
<p><strong>KittiBox: A car detection model implemented in Tensorflow.</strong></p>
<ul>
<li>keywords: MultiNet</li>
<li>intro: KittiBox is a collection of scripts to train out model FastBox on the Kitti Object Detection Dataset</li>
<li>github: <a href="https://github.com/MarvinTeichmann/KittiBox" target="_blank" rel="external">https://github.com/MarvinTeichmann/KittiBox</a></li>
</ul>
<p><strong>Deformable Convolutional Networks + MST + Soft-NMS</strong></p>
<ul>
<li>github: <a href="https://github.com/bharatsingh430/Deformable-ConvNets" target="_blank" rel="external">https://github.com/bharatsingh430/Deformable-ConvNets</a></li>
</ul>
<p><strong>How to Build a Real-time Hand-Detector using Neural Networks (SSD) on Tensorflow</strong></p>
<ul>
<li>blog: <a href="https://towardsdatascience.com/how-to-build-a-real-time-hand-detector-using-neural-networks-ssd-on-tensorflow-d6bac0e4b2ce" target="_blank" rel="external">https://towardsdatascience.com/how-to-build-a-real-time-hand-detector-using-neural-networks-ssd-on-tensorflow-d6bac0e4b2ce</a></li>
<li>github: <a href="https://github.com//victordibia/handtracking" target="_blank" rel="external">https://github.com//victordibia/handtracking</a></li>
</ul>
<h1 id="Leaderboard"><a href="#Leaderboard" class="headerlink" title="Leaderboard"></a>Leaderboard</h1><p><strong>Detection Results: VOC2012</strong></p>
<ul>
<li>intro: Competition “comp4” (train on additional data)</li>
<li>homepage: <a href="http://host.robots.ox.ac.uk:8080/leaderboard/displaylb.php?challengeid=11&amp;compid=4" target="_blank" rel="external">http://host.robots.ox.ac.uk:8080/leaderboard/displaylb.php?challengeid=11&amp;compid=4</a></li>
</ul>
<h1 id="Tools"><a href="#Tools" class="headerlink" title="Tools"></a>Tools</h1><p><strong>BeaverDam: Video annotation tool for deep learning training labels</strong></p>
<p><a href="https://github.com/antingshen/BeaverDam" target="_blank" rel="external">https://github.com/antingshen/BeaverDam</a></p>
<h1 id="Blogs"><a href="#Blogs" class="headerlink" title="Blogs"></a>Blogs</h1><p><strong>Convolutional Neural Networks for Object Detection</strong></p>
<p><a href="http://rnd.azoft.com/convolutional-neural-networks-object-detection/" target="_blank" rel="external">http://rnd.azoft.com/convolutional-neural-networks-object-detection/</a></p>
<p><strong>Introducing automatic object detection to visual search (Pinterest)</strong></p>
<ul>
<li>keywords: Faster R-CNN</li>
<li>blog: <a href="https://engineering.pinterest.com/blog/introducing-automatic-object-detection-visual-search" target="_blank" rel="external">https://engineering.pinterest.com/blog/introducing-automatic-object-detection-visual-search</a></li>
<li>demo: <a href="https://engineering.pinterest.com/sites/engineering/files/Visual%20Search%20V1%20-%20Video.mp4" target="_blank" rel="external">https://engineering.pinterest.com/sites/engineering/files/Visual%20Search%20V1%20-%20Video.mp4</a></li>
<li>review: <a href="https://news.developer.nvidia.com/pinterest-introduces-the-future-of-visual-search/?mkt_tok=eyJpIjoiTnpaa01UWXpPRE0xTURFMiIsInQiOiJJRjcybjkwTmtmallORUhLOFFFODBDclFqUlB3SWlRVXJXb1MrQ013TDRIMGxLQWlBczFIeWg0TFRUdnN2UHY2ZWFiXC9QQVwvQzBHM3B0UzBZblpOSmUyU1FcLzNPWXI4cml2VERwTTJsOFwvOEk9In0%3D" target="_blank" rel="external">https://news.developer.nvidia.com/pinterest-introduces-the-future-of-visual-search/?mkt_tok=eyJpIjoiTnpaa01UWXpPRE0xTURFMiIsInQiOiJJRjcybjkwTmtmallORUhLOFFFODBDclFqUlB3SWlRVXJXb1MrQ013TDRIMGxLQWlBczFIeWg0TFRUdnN2UHY2ZWFiXC9QQVwvQzBHM3B0UzBZblpOSmUyU1FcLzNPWXI4cml2VERwTTJsOFwvOEk9In0%3D</a></li>
</ul>
<p><strong>Deep Learning for Object Detection with DIGITS</strong></p>
<ul>
<li>blog: <a href="https://devblogs.nvidia.com/parallelforall/deep-learning-object-detection-digits/" target="_blank" rel="external">https://devblogs.nvidia.com/parallelforall/deep-learning-object-detection-digits/</a></li>
</ul>
<p><strong>Analyzing The Papers Behind Facebook’s Computer Vision Approach</strong></p>
<ul>
<li>keywords: DeepMask, SharpMask, MultiPathNet</li>
<li>blog: <a href="https://adeshpande3.github.io/adeshpande3.github.io/Analyzing-the-Papers-Behind-Facebook&#39;s-Computer-Vision-Approach/" target="_blank" rel="external">https://adeshpande3.github.io/adeshpande3.github.io/Analyzing-the-Papers-Behind-Facebook’s-Computer-Vision-Approach/</a></li>
</ul>
<p><strong>Easily Create High Quality Object Detectors with Deep Learning</strong></p>
<ul>
<li>intro: dlib v19.2</li>
<li>blog: <a href="http://blog.dlib.net/2016/10/easily-create-high-quality-object.html" target="_blank" rel="external">http://blog.dlib.net/2016/10/easily-create-high-quality-object.html</a></li>
</ul>
<p><strong>How to Train a Deep-Learned Object Detection Model in the Microsoft Cognitive Toolkit</strong></p>
<ul>
<li>blog: <a href="https://blogs.technet.microsoft.com/machinelearning/2016/10/25/how-to-train-a-deep-learned-object-detection-model-in-cntk/" target="_blank" rel="external">https://blogs.technet.microsoft.com/machinelearning/2016/10/25/how-to-train-a-deep-learned-object-detection-model-in-cntk/</a></li>
<li>github: <a href="https://github.com/Microsoft/CNTK/tree/master/Examples/Image/Detection/FastRCNN" target="_blank" rel="external">https://github.com/Microsoft/CNTK/tree/master/Examples/Image/Detection/FastRCNN</a></li>
</ul>
<p><strong>Object Detection in Satellite Imagery, a Low Overhead Approach</strong></p>
<ul>
<li>part 1: <a href="https://medium.com/the-downlinq/object-detection-in-satellite-imagery-a-low-overhead-approach-part-i-cbd96154a1b7#.2csh4iwx9" target="_blank" rel="external">https://medium.com/the-downlinq/object-detection-in-satellite-imagery-a-low-overhead-approach-part-i-cbd96154a1b7#.2csh4iwx9</a></li>
<li>part 2: <a href="https://medium.com/the-downlinq/object-detection-in-satellite-imagery-a-low-overhead-approach-part-ii-893f40122f92#.f9b7dgf64" target="_blank" rel="external">https://medium.com/the-downlinq/object-detection-in-satellite-imagery-a-low-overhead-approach-part-ii-893f40122f92#.f9b7dgf64</a></li>
</ul>
<p><strong>You Only Look Twice — Multi-Scale Object Detection in Satellite Imagery With Convolutional Neural Networks</strong></p>
<ul>
<li>part 1: <a href="https://medium.com/the-downlinq/you-only-look-twice-multi-scale-object-detection-in-satellite-imagery-with-convolutional-neural-38dad1cf7571#.fmmi2o3of" target="_blank" rel="external">https://medium.com/the-downlinq/you-only-look-twice-multi-scale-object-detection-in-satellite-imagery-with-convolutional-neural-38dad1cf7571#.fmmi2o3of</a></li>
<li>part 2: <a href="https://medium.com/the-downlinq/you-only-look-twice-multi-scale-object-detection-in-satellite-imagery-with-convolutional-neural-34f72f659588#.nwzarsz1t" target="_blank" rel="external">https://medium.com/the-downlinq/you-only-look-twice-multi-scale-object-detection-in-satellite-imagery-with-convolutional-neural-34f72f659588#.nwzarsz1t</a></li>
</ul>
<p><strong>Faster R-CNN Pedestrian and Car Detection</strong></p>
<ul>
<li>blog: <a href="https://bigsnarf.wordpress.com/2016/11/07/faster-r-cnn-pedestrian-and-car-detection/" target="_blank" rel="external">https://bigsnarf.wordpress.com/2016/11/07/faster-r-cnn-pedestrian-and-car-detection/</a></li>
<li>ipn: <a href="https://gist.github.com/bigsnarfdude/2f7b2144065f6056892a98495644d3e0#file-demo_faster_rcnn_notebook-ipynb" target="_blank" rel="external">https://gist.github.com/bigsnarfdude/2f7b2144065f6056892a98495644d3e0#file-demo_faster_rcnn_notebook-ipynb</a></li>
<li>github: <a href="https://github.com/bigsnarfdude/Faster-RCNN_TF" target="_blank" rel="external">https://github.com/bigsnarfdude/Faster-RCNN_TF</a></li>
</ul>
<p><strong>Small U-Net for vehicle detection</strong></p>
<ul>
<li>blog: <a href="https://medium.com/@vivek.yadav/small-u-net-for-vehicle-detection-9eec216f9fd6#.md4u80kad" target="_blank" rel="external">https://medium.com/@vivek.yadav/small-u-net-for-vehicle-detection-9eec216f9fd6#.md4u80kad</a></li>
</ul>
<p><strong>Region of interest pooling explained</strong></p>
<ul>
<li>blog: <a href="https://deepsense.io/region-of-interest-pooling-explained/" target="_blank" rel="external">https://deepsense.io/region-of-interest-pooling-explained/</a></li>
<li>github: <a href="https://github.com/deepsense-io/roi-pooling" target="_blank" rel="external">https://github.com/deepsense-io/roi-pooling</a></li>
</ul>
<p><strong>Supercharge your Computer Vision models with the TensorFlow Object Detection API</strong></p>
<ul>
<li>blog: <a href="https://research.googleblog.com/2017/06/supercharge-your-computer-vision-models.html" target="_blank" rel="external">https://research.googleblog.com/2017/06/supercharge-your-computer-vision-models.html</a></li>
<li>github: <a href="https://github.com/tensorflow/models/tree/master/object_detection" target="_blank" rel="external">https://github.com/tensorflow/models/tree/master/object_detection</a></li>
</ul>
<p><strong>Understanding SSD MultiBox — Real-Time Object Detection In Deep Learning</strong></p>
<p><a href="https://towardsdatascience.com/understanding-ssd-multibox-real-time-object-detection-in-deep-learning-495ef744fab" target="_blank" rel="external">https://towardsdatascience.com/understanding-ssd-multibox-real-time-object-detection-in-deep-learning-495ef744fab</a></p>

      
    </div>
    
    
    

    

    
      <div>
        <div style="padding: 10px 0; margin: 20px auto; width: 90%; text-align: center;">
  <div>您的支持将鼓励我努力创作！</div>
  <button id="rewardButton" disable="enable" onclick="var qr = document.getElementById('QR'); if (qr.style.display === 'none') {qr.style.display='block';} else {qr.style.display='none'}">
    <span>打赏</span>
  </button>
  <div id="QR" style="display: none;">

    
      <div id="wechat" style="display: inline-block">
        <img id="wechat_qr" src="/images/wechatpay.jpg" alt="Dreamsong 微信支付"/>
        <p>微信支付</p>
      </div>
    

    
      <div id="alipay" style="display: inline-block">
        <img id="alipay_qr" src="/images/alipay.jpg" alt="Dreamsong 支付宝"/>
        <p>支付宝</p>
      </div>
    

    

  </div>
</div>

      </div>
    

    
      <div>
        <ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者：</strong>
    Dreamsong
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="http://songit.cn/Object-Detection/" title="Object-Detection">http://songit.cn/Object-Detection/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>
    本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-nd/4.0/deed.zh" rel="external nofollow" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明出处！
  </li>
</ul>

      </div>
    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Github/" rel="tag"># Github</a>
          
            <a href="/tags/DeepLearning/" rel="tag"># DeepLearning</a>
          
            <a href="/tags/ObjectDetection/" rel="tag"># ObjectDetection</a>
          
            <a href="/tags/Paper/" rel="tag"># Paper</a>
          
        </div>
      

      
      
        <div class="post-widgets">
        

        

        
          
          <div id="needsharebutton-postbottom">
            <span class="btn">
              <i class="fa fa-share-alt" aria-hidden="true"></i>
            </span>
          </div>
        
        </div>
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/Datasets/" rel="next" title="数据集（持续更新）">
                <i class="fa fa-chevron-left"></i> 数据集（持续更新）
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/RunFaster-R-cnn-TF-OnUbuntu/" rel="prev" title="Ubuntu16.04+CUDA9.0+CUDNN7.0+Faster RCNN+Tensorflow1.5">
                Ubuntu16.04+CUDA9.0+CUDNN7.0+Faster RCNN+Tensorflow1.5 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
      <div id="SOHUCS"></div>
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/avatar.jpg"
                alt="Dreamsong" />
            
              <p class="site-author-name" itemprop="name">Dreamsong</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">40</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">5</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">40</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          <div class="links-of-author motion-element">
            
              
                <span class="links-of-author-item">
                  <a href="https://github.com/dreamay" target="_blank" title="GitHub">
                    
                      <i class="fa fa-fw fa-github"></i>GitHub</a>
                </span>
              
                <span class="links-of-author-item">
                  <a href="mailto:me@songit.cn" target="_blank" title="E-Mail">
                    
                      <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                </span>
              
                <span class="links-of-author-item">
                  <a href="http://weibo.com/u/2204471217" target="_blank" title="Weibo">
                    
                      <i class="fa fa-fw fa-weibo"></i>Weibo</a>
                </span>
              
                <span class="links-of-author-item">
                  <a href="http://www.jianshu.com/u/152629b84b6a" target="_blank" title="Jianshu">
                    
                      <i class="fa fa-fw fa-book"></i>Jianshu</a>
                </span>
              
            
          </div>

          
          

          
          
            <div class="links-of-blogroll motion-element links-of-blogroll-block">
              <div class="links-of-blogroll-title">
                <i class="fa  fa-fw fa-link"></i>
                友情链接
              </div>
              <ul class="links-of-blogroll-list">
                
                  <li class="links-of-blogroll-item">
                    <a href="http://www.mxsong.com/" title="My Homepage" target="_blank">My Homepage</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="http://iamlay.com/" title="AimLay的技术博客" target="_blank">AimLay的技术博客</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="http://sovtan.com/" title="坦坦的小屋" target="_blank">坦坦的小屋</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="https://www.guru99.com/selenium-tutorial.html" title="Selenium" target="_blank">Selenium</a>
                  </li>
                
              </ul>
            </div>
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Papers"><span class="nav-number">1.</span> <span class="nav-text">Papers</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#R-CNN"><span class="nav-number">1.1.</span> <span class="nav-text">R-CNN</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Fast-R-CNN"><span class="nav-number">1.2.</span> <span class="nav-text">Fast R-CNN</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Faster-R-CNN"><span class="nav-number">1.3.</span> <span class="nav-text">Faster R-CNN</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Light-Head-R-CNN"><span class="nav-number">1.4.</span> <span class="nav-text">Light-Head R-CNN</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Cascade-R-CNN"><span class="nav-number">1.5.</span> <span class="nav-text">Cascade R-CNN</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#MultiBox"><span class="nav-number">1.6.</span> <span class="nav-text">MultiBox</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#SPP-Net"><span class="nav-number">1.7.</span> <span class="nav-text">SPP-Net</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#MR-CNN"><span class="nav-number">1.8.</span> <span class="nav-text">MR-CNN</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#YOLO"><span class="nav-number">1.9.</span> <span class="nav-text">YOLO</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#YOLOv2"><span class="nav-number">1.10.</span> <span class="nav-text">YOLOv2</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#YOLOv3"><span class="nav-number">2.</span> <span class="nav-text">YOLOv3</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#DenseBox"><span class="nav-number">2.1.</span> <span class="nav-text">DenseBox</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#SSD"><span class="nav-number">2.2.</span> <span class="nav-text">SSD</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#DSSD"><span class="nav-number">2.3.</span> <span class="nav-text">DSSD</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#FSSD"><span class="nav-number">2.4.</span> <span class="nav-text">FSSD</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ESSD"><span class="nav-number">2.5.</span> <span class="nav-text">ESSD</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Inside-Outside-Net-ION"><span class="nav-number">2.6.</span> <span class="nav-text">Inside-Outside Net (ION)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#CRAFT"><span class="nav-number">2.7.</span> <span class="nav-text">CRAFT</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#OHEM"><span class="nav-number">2.8.</span> <span class="nav-text">OHEM</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#R-FCN"><span class="nav-number">2.9.</span> <span class="nav-text">R-FCN</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#MS-CNN"><span class="nav-number">2.10.</span> <span class="nav-text">MS-CNN</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#PVANET"><span class="nav-number">2.11.</span> <span class="nav-text">PVANET</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#GBD-Net"><span class="nav-number">2.12.</span> <span class="nav-text">GBD-Net</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Feature-Pyramid-Network-FPN"><span class="nav-number">2.13.</span> <span class="nav-text">Feature Pyramid Network (FPN)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#DSOD"><span class="nav-number">2.14.</span> <span class="nav-text">DSOD</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#RetinaNet"><span class="nav-number">2.15.</span> <span class="nav-text">RetinaNet</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#MegDet"><span class="nav-number">2.16.</span> <span class="nav-text">MegDet</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Non-Maximum-Suppression-NMS"><span class="nav-number">3.</span> <span class="nav-text">Non-Maximum Suppression (NMS)</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Adversarial-Examples"><span class="nav-number">4.</span> <span class="nav-text">Adversarial Examples</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Weakly-Supervised-Object-Detection"><span class="nav-number">5.</span> <span class="nav-text">Weakly Supervised Object Detection</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Video-Object-Detection"><span class="nav-number">6.</span> <span class="nav-text">Video Object Detection</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Object-Detection-in-3D"><span class="nav-number">7.</span> <span class="nav-text">Object Detection in 3D</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Object-Detection-on-RGB-D"><span class="nav-number">8.</span> <span class="nav-text">Object Detection on RGB-D</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Salient-Object-Detection"><span class="nav-number">9.</span> <span class="nav-text">Salient Object Detection</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Video-Saliency-Detection"><span class="nav-number">10.</span> <span class="nav-text">Video Saliency Detection</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Visual-Relationship-Detection"><span class="nav-number">11.</span> <span class="nav-text">Visual Relationship Detection</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Face-Deteciton"><span class="nav-number">12.</span> <span class="nav-text">Face Deteciton</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#MTCNN"><span class="nav-number">12.1.</span> <span class="nav-text">MTCNN</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Detect-Small-Faces"><span class="nav-number">12.2.</span> <span class="nav-text">Detect Small Faces</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Person-Head-Detection"><span class="nav-number">13.</span> <span class="nav-text">Person Head Detection</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Pedestrian-Detection-People-Detection"><span class="nav-number">14.</span> <span class="nav-text">Pedestrian Detection / People Detection</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Vehicle-Detection"><span class="nav-number">15.</span> <span class="nav-text">Vehicle Detection</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Traffic-Sign-Detection"><span class="nav-number">16.</span> <span class="nav-text">Traffic-Sign Detection</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Skeleton-Detection"><span class="nav-number">17.</span> <span class="nav-text">Skeleton Detection</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Fruit-Detection"><span class="nav-number">18.</span> <span class="nav-text">Fruit Detection</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Shadow-Detection"><span class="nav-number">18.1.</span> <span class="nav-text">Shadow Detection</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Others-Deteciton"><span class="nav-number">19.</span> <span class="nav-text">Others Deteciton</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Object-Proposal"><span class="nav-number">20.</span> <span class="nav-text">Object Proposal</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Localization"><span class="nav-number">21.</span> <span class="nav-text">Localization</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Tutorials-Talks"><span class="nav-number">22.</span> <span class="nav-text">Tutorials / Talks</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Projects"><span class="nav-number">23.</span> <span class="nav-text">Projects</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Leaderboard"><span class="nav-number">24.</span> <span class="nav-text">Leaderboard</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Tools"><span class="nav-number">25.</span> <span class="nav-text">Tools</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Blogs"><span class="nav-number">26.</span> <span class="nav-text">Blogs</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; 2016 &mdash; <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Dreamsong</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <a href="http://www.beian.miit.gov.cn/state/outPortal/loginPortal.action">豫ICP备17030278号</a>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a></div>




        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      <i class="fa fa-user"></i> 本站总访客
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      人
    </span>
  

  
    <span class="site-pv">
      <i class="fa fa-eye"></i> 总访问量
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      次
    </span>
  
</div>








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.3"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.3"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.3"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.3"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.3"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.3"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.3"></script>



  


  




	





  





  




  
    <script type="text/javascript">
    (function(){
      var appid = 'cytamnzhz';
      var conf = 'c51e63345c8bbfaa2bf7d0267f93f04c';
      var width = window.innerWidth || document.documentElement.clientWidth;
      if (width < 960) {
      window.document.write('<script id="changyan_mobile_js" charset="utf-8" type="text/javascript" src="https://changyan.sohu.com/upload/mobile/wap-js/changyan_mobile.js?client_id=' + appid + '&conf=' + conf + '"><\/script>'); } else { var loadJs=function(d,a){var c=document.getElementsByTagName("head")[0]||document.head||document.documentElement;var b=document.createElement("script");b.setAttribute("type","text/javascript");b.setAttribute("charset","UTF-8");b.setAttribute("src",d);if(typeof a==="function"){if(window.attachEvent){b.onreadystatechange=function(){var e=b.readyState;if(e==="loaded"||e==="complete"){b.onreadystatechange=null;a()}}}else{b.onload=a}}c.appendChild(b)};loadJs("https://changyan.sohu.com/upload/changyan.js",function(){
        window.changyan.api.config({appid:appid,conf:conf})});
      }
    })();
    </script>
    <script type="text/javascript" src="https://assets.changyan.sohu.com/upload/plugins/plugins.count.js"></script>
  






<script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
<script src="//unpkg.com/valine/dist/Valine.min.js"></script>



  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  
  
  
  <link rel="stylesheet" href="/lib/needsharebutton/needsharebutton.css">

  
  
  <script src="/lib/needsharebutton/needsharebutton.js"></script>

  <script>
    
      pbOptions = {};
      
          pbOptions.iconStyle = "box";
      
          pbOptions.boxForm = "horizontal";
      
          pbOptions.position = "bottomCenter";
      
          pbOptions.networks = "Weibo,Wechat,Douban,QQZone,Twitter,Facebook";
      
      new needShareButton('#needsharebutton-postbottom', pbOptions);
    
    
  </script>

  

  

  

  

</body>
</html>
